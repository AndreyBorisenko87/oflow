"""
–ë–ª–æ–∫ 11: –ö–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–π —Å–ª–æ–π
–°–æ—Ö—Ä–∞–Ω–∏—Ç—å quotes/book_top/tape/nbbo/basis/features –ø–∞—á–∫–∞–º–∏ –ø–æ –¥–Ω—è–º
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional
from pathlib import Path
import logging
import time
import json
from datetime import datetime, timedelta
import hashlib

logger = logging.getLogger(__name__)

class CanonicalStorage:
    """–ö–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–π —Å–ª–æ–π –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–∞—á–∫–∞–º–∏ –ø–æ –¥–Ω—è–º"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.batch_size_days = config.get('batch', {}).get('days', 1)
        self.compression = config.get('compression', {}).get('type', 'snappy')
        self.index_enabled = config.get('indexing', {}).get('enabled', True)
        self.metadata_enabled = config.get('metadata', {}).get('enabled', True)
        self.chunk_size = config.get('batch', {}).get('chunk_size', 10000)
        
        # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö
        self.supported_types = [
            'quotes', 'book_top', 'tape', 'nbbo', 
            'basis', 'features', 'events', 'trades', 'depth'
        ]
        
    def run_canonical_storage(
        self,
        data_dict: Dict[str, pd.DataFrame],
        output_dir: Path
    ) -> Dict[str, List[Path]]:
        """–ü–æ–ª–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–π —Å–ª–æ–π"""
        start_time = time.time()
        logger.info("=== –ù–∞—á–∞–ª–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–π —Å–ª–æ–π ===")
        
        # –≠—Ç–∞–ø 1: –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        logger.info("–≠—Ç–∞–ø 1/4: –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        validated_data = self._validate_data(data_dict)
        
        # –≠—Ç–∞–ø 2: –†–∞–∑–±–∏–≤–∫–∞ –ø–æ –¥–Ω—è–º
        logger.info("–≠—Ç–∞–ø 2/4: –†–∞–∑–±–∏–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ –¥–Ω—è–º...")
        batched_data = self._batch_by_days(validated_data)
        
        # –≠—Ç–∞–ø 3: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–∞—á–∫–∞–º–∏
        logger.info("–≠—Ç–∞–ø 3/4: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø–∞—á–∫–∞–º–∏...")
        saved_files = self._save_batches(batched_data, output_dir)
        
        # –≠—Ç–∞–ø 4: –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        logger.info("–≠—Ç–∞–ø 4/4: –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö...")
        self._create_indexes_and_metadata(saved_files, output_dir)
        
        # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ
        duration = time.time() - start_time
        total_files = sum(len(files) for files in saved_files.values())
        logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {duration:.2f}—Å")
        logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {total_files} —Ñ–∞–π–ª–æ–≤ –ø–æ {len(saved_files)} —Ç–∏–ø–∞–º –¥–∞–Ω–Ω—ã—Ö")
        logger.info("=== –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–π —Å–ª–æ–π –∑–∞–≤–µ—Ä—à–µ–Ω–æ ===")
        
        return saved_files
    
    def _validate_data(self, data_dict: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        logger.info("–í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        
        validated_data = {}
        
        for data_type, df in data_dict.items():
            if data_type not in self.supported_types:
                logger.warning(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö: {data_type}")
                continue
            
            if df.empty:
                logger.warning(f"–ü—É—Å—Ç–æ–π DataFrame –¥–ª—è —Ç–∏–ø–∞: {data_type}")
                continue
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
            required_columns = self._get_required_columns(data_type)
            missing_columns = [col for col in required_columns if col not in df.columns]
            
            if missing_columns:
                logger.warning(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è {data_type}: {missing_columns}")
                continue
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∫–æ–ª–æ–Ω–∫–∏
            if 'ts_ns' in df.columns:
                df = df.sort_values('ts_ns').reset_index(drop=True)
                df['date'] = pd.to_datetime(df['ts_ns'], unit='ns').dt.date
                validated_data[data_type] = df
                logger.info(f"–í–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω {data_type}: {len(df)} –∑–∞–ø–∏—Å–µ–π")
            else:
                logger.warning(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω–∞—è –∫–æ–ª–æ–Ω–∫–∞ ts_ns –¥–ª—è {data_type}")
        
        return validated_data
    
    def _get_required_columns(self, data_type: str) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è —Ç–∏–ø–∞ –¥–∞–Ω–Ω—ã—Ö"""
        required_columns_map = {
            'quotes': ['ts_ns', 'exchange', 'symbol', 'bid', 'ask'],
            'book_top': ['ts_ns', 'exchange', 'symbol', 'side', 'price', 'size'],
            'tape': ['ts_ns', 'exchange', 'symbol', 'aggression_side'],
            'nbbo': ['ts_ns', 'best_bid', 'best_ask'],
            'basis': ['ts_ns', 'spot_symbol', 'futures_symbol', 'basis_abs'],
            'features': ['ts_ns', 'exchange', 'symbol'],
            'events': ['ts_ns', 'exchange', 'symbol', 'pattern_type'],
            'trades': ['ts_ns', 'exchange', 'symbol', 'price', 'size'],
            'depth': ['ts_ns', 'exchange', 'symbol', 'side', 'price', 'size']
        }
        return required_columns_map.get(data_type, ['ts_ns'])
    
    def _batch_by_days(self, data_dict: Dict[str, pd.DataFrame]) -> Dict[str, Dict[str, List[pd.DataFrame]]]:
        """–†–∞–∑–±–∏–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ –¥–Ω—è–º"""
        logger.info(f"–†–∞–∑–±–∏–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ –¥–Ω—è–º (—Ä–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞: {self.batch_size_days} –¥–Ω–µ–π)...")
        
        batched_data = {}
        
        for data_type, df in data_dict.items():
            if df.empty:
                continue
            
            # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–∞—Ç–∞–º
            date_groups = df.groupby('date')
            logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {data_type}: {len(date_groups)} –¥–Ω–µ–π –¥–∞–Ω–Ω—ã—Ö")
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–∫–µ—Ç–æ–≤
            batches = {}
            dates = sorted(df['date'].unique())
            
            # –ü—Ä–æ–≥—Ä–µ—Å—Å-–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
            total_dates = len(dates)
            for i, date in enumerate(dates):
                if i % 10 == 0:  # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–µ 10 –¥–Ω–µ–π
                    logger.info(f"–ü—Ä–æ–≥—Ä–µ—Å—Å {data_type}: {i+1}/{total_dates} –¥–Ω–µ–π")
                
                # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞–∫–µ—Ç–∞
                batch_key = self._get_batch_key(date, self.batch_size_days)
                
                if batch_key not in batches:
                    batches[batch_key] = []
                
                # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–Ω—è –≤ –ø–∞–∫–µ—Ç
                day_data = date_groups.get_group(date).copy()
                day_data = day_data.drop(columns=['date'])  # –£–±–∏—Ä–∞–µ–º —Å–ª—É–∂–µ–±–Ω—É—é –∫–æ–ª–æ–Ω–∫—É
                batches[batch_key].append(day_data)
            
            # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–∞–∫–µ—Ç–æ–≤
            final_batches = {}
            for batch_key, day_dfs in batches.items():
                if day_dfs:
                    combined_df = pd.concat(day_dfs, ignore_index=True)
                    combined_df = combined_df.sort_values('ts_ns').reset_index(drop=True)
                    final_batches[batch_key] = [combined_df]  # –û–¥–∏–Ω –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π DataFrame –Ω–∞ –ø–∞–∫–µ—Ç
            
            batched_data[data_type] = final_batches
            logger.info(f"–°–æ–∑–¥–∞–Ω–æ {len(final_batches)} –ø–∞–∫–µ—Ç–æ–≤ –¥–ª—è {data_type}")
        
        return batched_data
    
    def _get_batch_key(self, date, batch_size_days: int) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–ª—é—á–∞ –ø–∞–∫–µ—Ç–∞ –¥–ª—è –¥–∞—Ç—ã"""
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ datetime –¥–ª—è –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–∏
        if isinstance(date, str):
            date = pd.to_datetime(date).date()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞—á–∞–ª–æ —ç–ø–æ—Ö–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 2020-01-01)
        epoch_start = datetime(2020, 1, 1).date()
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –æ—Ç –Ω–∞—á–∞–ª–∞ —ç–ø–æ—Ö–∏
        days_since_epoch = (date - epoch_start).days
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–æ–º–µ—Ä –ø–∞–∫–µ—Ç–∞
        batch_number = days_since_epoch // batch_size_days
        
        # –í—ã—á–∏—Å–ª—è–µ–º –≥—Ä–∞–Ω–∏—Ü—ã –ø–∞–∫–µ—Ç–∞
        batch_start_days = batch_number * batch_size_days
        batch_start_date = epoch_start + timedelta(days=batch_start_days)
        batch_end_date = batch_start_date + timedelta(days=batch_size_days - 1)
        
        return f"{batch_start_date.strftime('%Y%m%d')}_{batch_end_date.strftime('%Y%m%d')}"
    
    def _save_batches(
        self,
        batched_data: Dict[str, Dict[str, List[pd.DataFrame]]],
        output_dir: Path
    ) -> Dict[str, List[Path]]:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–∞–∫–µ—Ç–æ–≤ –¥–∞–Ω–Ω—ã—Ö"""
        logger.info("–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–∞–∫–µ—Ç–æ–≤ –¥–∞–Ω–Ω—ã—Ö...")
        
        saved_files = {}
        
        for data_type, batches in batched_data.items():
            saved_files[data_type] = []
            
            logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ {data_type}: {len(batches)} –ø–∞–∫–µ—Ç–æ–≤")
            
            for i, (batch_key, batch_dfs) in enumerate(batches.items()):
                if i % 5 == 0:  # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–µ 5 –ø–∞–∫–µ—Ç–æ–≤
                    logger.info(f"–ü—Ä–æ–≥—Ä–µ—Å—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è {data_type}: {i+1}/{len(batches)} –ø–∞–∫–µ—Ç–æ–≤")
                
                for j, df in enumerate(batch_dfs):
                    if df.empty:
                        continue
                    
                    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—É—Ç–∏ —Ñ–∞–π–ª–∞
                    filename = f"{data_type}_{batch_key}"
                    if len(batch_dfs) > 1:
                        filename += f"_part{j+1:03d}"
                    filename += ".parquet"
                    
                    file_path = output_dir / data_type / filename
                    file_path.parent.mkdir(parents=True, exist_ok=True)
                    
                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ —Å–∂–∞—Ç–∏—è
                    df.to_parquet(
                        file_path,
                        engine="pyarrow",
                        compression=self.compression,
                        index=False
                    )
                    
                    saved_files[data_type].append(file_path)
                    logger.debug(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω {file_path}: {len(df)} –∑–∞–ø–∏—Å–µ–π")
            
            logger.info(f"–ó–∞–≤–µ—Ä—à–µ–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ {data_type}: {len(saved_files[data_type])} —Ñ–∞–π–ª–æ–≤")
        
        return saved_files
    
    def _create_indexes_and_metadata(
        self,
        saved_files: Dict[str, List[Path]],
        output_dir: Path
    ) -> None:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
        logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö...")
        
        if not self.index_enabled and not self.metadata_enabled:
            logger.info("–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ—Ç–∫–ª—é—á–µ–Ω—ã")
            return
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—â–µ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
        if self.index_enabled:
            index_data = self._create_file_index(saved_files)
            index_path = output_dir / "file_index.json"
            
            with open(index_path, 'w', encoding='utf-8') as f:
                json.dump(index_data, f, indent=2, default=str, ensure_ascii=False)
            
            logger.info(f"–ò–Ω–¥–µ–∫—Å —Ñ–∞–π–ª–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {index_path}")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        if self.metadata_enabled:
            metadata = self._create_metadata(saved_files)
            metadata_path = output_dir / "metadata.json"
            
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, default=str, ensure_ascii=False)
            
            logger.info(f"–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {metadata_path}")
    
    def _create_file_index(self, saved_files: Dict[str, List[Path]]) -> Dict:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ —Ñ–∞–π–ª–æ–≤"""
        index_data = {
            "created_at": datetime.now().isoformat(),
            "total_files": sum(len(files) for files in saved_files.values()),
            "data_types": list(saved_files.keys()),
            "files": {}
        }
        
        for data_type, files in saved_files.items():
            index_data["files"][data_type] = []
            
            for file_path in files:
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
                filename = file_path.name
                file_info = {
                    "path": str(file_path.relative_to(file_path.parent.parent)),
                    "filename": filename,
                    "size_bytes": file_path.stat().st_size if file_path.exists() else 0,
                    "created_at": datetime.fromtimestamp(file_path.stat().st_mtime).isoformat() if file_path.exists() else None
                }
                
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –≥—Ä–∞–Ω–∏—Ü –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
                if "_" in filename:
                    try:
                        parts = filename.replace(".parquet", "").split("_")
                        if len(parts) >= 3:
                            start_date = parts[1]
                            end_date = parts[2]
                            file_info["date_range"] = {
                                "start": start_date,
                                "end": end_date
                            }
                    except Exception as e:
                        logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã –∏–∑ {filename}: {e}")
                
                index_data["files"][data_type].append(file_info)
        
        return index_data
    
    def _create_metadata(self, saved_files: Dict[str, List[Path]]) -> Dict:
        """–°–æ–∑–¥–∞–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
        metadata = {
            "version": "1.0",
            "created_at": datetime.now().isoformat(),
            "config": self.config,
            "statistics": {},
            "schema": {}
        }
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º –¥–∞–Ω–Ω—ã—Ö
        for data_type, files in saved_files.items():
            total_size = sum(
                file_path.stat().st_size 
                for file_path in files 
                if file_path.exists()
            )
            
            metadata["statistics"][data_type] = {
                "file_count": len(files),
                "total_size_bytes": total_size,
                "total_size_mb": round(total_size / 1024 / 1024, 2)
            }
        
        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_files = sum(len(files) for files in saved_files.values())
        total_size = sum(
            stat["total_size_bytes"] 
            for stat in metadata["statistics"].values()
        )
        
        metadata["summary"] = {
            "total_files": total_files,
            "total_size_bytes": total_size,
            "total_size_mb": round(total_size / 1024 / 1024, 2),
            "compression": self.compression,
            "batch_size_days": self.batch_size_days
        }
        
        return metadata

def batch_by_days(
    data_dict: Dict[str, pd.DataFrame],
    batch_size_days: int = 1
) -> Dict[str, List[pd.DataFrame]]:
    """–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ä–∞–∑–±–∏–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –ø–æ –¥–Ω—è–º (legacy)"""
    logger.info(f"–†–∞–∑–±–∏–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ –¥–Ω—è–º (—Ä–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞: {batch_size_days})...")
    
    config = {
        'batch': {'days': batch_size_days},
        'compression': {'type': 'snappy'},
        'indexing': {'enabled': False},
        'metadata': {'enabled': False}
    }
    
    storage = CanonicalStorage(config)
    validated_data = storage._validate_data(data_dict)
    batched_data = storage._batch_by_days(validated_data)
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ legacy —Ñ–æ—Ä–º–∞—Ç
    legacy_batches = {}
    for data_type, batches in batched_data.items():
        legacy_batches[data_type] = []
        for batch_key, batch_dfs in batches.items():
            legacy_batches[data_type].extend(batch_dfs)
    
    return legacy_batches

def save_canonical(
    data_dict: Dict[str, pd.DataFrame],
    output_dir: Path,
    config: Dict
) -> Dict[str, List[Path]]:
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""
    logger.info("–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö...")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
    storage = CanonicalStorage(config)
    
    # –ü–æ–ª–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–µ–π –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
    return storage.run_canonical_storage(data_dict, output_dir)

def run_canonical(
    data_dict: Dict[str, pd.DataFrame],
    config: Dict,
    output_dir: Path = Path("data/canon")
) -> Dict[str, List[Path]]:
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–æ–≥–æ —Å–ª–æ—è"""
    logger.info("–ó–∞–ø—É—Å–∫ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–æ–≥–æ —Å–ª–æ—è...")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    saved_files = save_canonical(data_dict, output_dir, config)
    
    logger.info("–ö–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–π —Å–ª–æ–π –∑–∞–≤–µ—Ä—à–µ–Ω")
    return saved_files


if __name__ == "__main__":
    """–ó–∞–ø—É—Å–∫ –±–ª–æ–∫–∞ 11 –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    import pandas as pd
    import logging
    import sys
    import os
    import time
    
    # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ base_block
    sys.path.append(os.path.dirname(__file__))
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')
    
    print("üß™ –¢–ï–°–¢ –ë–õ–û–ö–ê 11: Canonical")
    print("=" * 50)
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –±–ª–æ–∫–æ–≤
        quotes_df = pd.read_parquet("../../../data/quotes/quotes.parquet")
        book_df = pd.read_parquet("../../../data/test_small/book_top_small.parquet")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ
        data_dict = {}
        
        # Quotes
        if not quotes_df.empty:
            data_dict['quotes'] = quotes_df
            print(f"   - quotes: {len(quotes_df)} —Å—Ç—Ä–æ–∫")
        
        # Book top
        if not book_df.empty:
            data_dict['book_top'] = book_df
            print(f"   - book_top: {len(book_df)} —Å—Ç—Ä–æ–∫")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—Ä—É–≥–∏–µ –¥–∞–Ω–Ω—ã–µ
        try:
            tape_df = pd.read_parquet("../../../data/tape/tape_analyzed.parquet")
            if not tape_df.empty:
                data_dict['tape'] = tape_df
                print(f"   - tape: {len(tape_df)} —Å—Ç—Ä–æ–∫")
        except:
            print("   - tape: –¥–∞–Ω–Ω—ã–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")
        
        try:
            nbbo_df = pd.read_parquet("../../../data/nbbo/nbbo_aggregated.parquet")
            if not nbbo_df.empty:
                data_dict['nbbo'] = nbbo_df
                print(f"   - nbbo: {len(nbbo_df)} —Å—Ç—Ä–æ–∫")
        except:
            print("   - nbbo: –¥–∞–Ω–Ω—ã–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")
        
        try:
            basis_df = pd.read_parquet("../../../data/basis/basis_analyzed.parquet")
            if not basis_df.empty:
                data_dict['basis'] = basis_df
                print(f"   - basis: {len(basis_df)} —Å—Ç—Ä–æ–∫")
        except:
            print("   - basis: –¥–∞–Ω–Ω—ã–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")
        
        try:
            features_df = pd.read_parquet("../../../data/features/detector_features.parquet")
            if not features_df.empty:
                data_dict['features'] = features_df
                print(f"   - features: {len(features_df)} —Å—Ç—Ä–æ–∫")
        except:
            print("   - features: –¥–∞–Ω–Ω—ã–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â—É—é –∫–æ–ª–æ–Ω–∫—É instrument –≤ quotes
        if 'instrument' not in quotes_df.columns:
            quotes_df['instrument'] = 'ETHUSDT'
        
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–æ–≥–æ —Å–ª–æ—è:")
        for data_type, df in data_dict.items():
            if not df.empty:
                print(f"   - {data_type}: {len(df)} —Å—Ç—Ä–æ–∫")
            else:
                print(f"   - {data_type}: –ø—É—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ")
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–æ–≥–æ —Å–ª–æ—è
        config = {
            'batch': {
                'days': 1,           # –†–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ –≤ –¥–Ω—è—Ö
                'chunk_size': 10000  # –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
            },
            'compression': {
                'type': 'snappy'     # –¢–∏–ø —Å–∂–∞—Ç–∏—è
            },
            'indexing': {
                'enabled': True      # –í–∫–ª—é—á–∏—Ç—å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é
            },
            'metadata': {
                'enabled': True      # –í–∫–ª—é—á–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            }
        }
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –±–ª–æ–∫ 11
        print("üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º –±–ª–æ–∫ 11: –ö–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–π —Å–ª–æ–π...")
        start_time = time.time()
        
        saved_files = run_canonical(data_dict, config)
        
        execution_time = time.time() - start_time
        
        print(f"‚úÖ –ë–ª–æ–∫ 11 –≤—ã–ø–æ–ª–Ω–µ–Ω –∑–∞ {execution_time:.2f} —Å–µ–∫—É–Ω–¥!")
        print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–æ–≥–æ —Å–ª–æ—è:")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        total_files = sum(len(files) for files in saved_files.values())
        print(f"   - –í—Å–µ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {total_files}")
        
        for data_type, files in saved_files.items():
            print(f"   - {data_type}: {len(files)} —Ñ–∞–π–ª–æ–≤")
        
        print("‚úÖ –ë–ª–æ–∫ 11 —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω!")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
