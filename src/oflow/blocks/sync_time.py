"""
–ë–ª–æ–∫ 04: –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–∏
–û—Ü–µ–Ω–∏—Ç—å –∏ —Å–¥–≤–∏–Ω—É—Ç—å –ª–∞–≥–∏ –ø–æ –±–∏—Ä–∂–∞–º/–∏—Å—Ç–æ—á–Ω–∏–∫–∞–º –∏ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –¥—Ä–æ–ø—ã
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple
from pathlib import Path
import logging

logger = logging.getLogger(__name__)

def estimate_exchange_lags(trades_df: pd.DataFrame, depth_df: pd.DataFrame, config: Dict = None) -> Dict[str, int]:
    """
    –û—Ü–µ–Ω–∏—Ç—å –ª–∞–≥–∏ –º–µ–∂–¥—É –±–∏—Ä–∂–∞–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ trades –¥–∞–Ω–Ω—ã—Ö
    
    Args:
        trades_df: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ trades –¥–∞–Ω–Ω—ã–µ
        depth_df: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ depth –¥–∞–Ω–Ω—ã–µ
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –ª–∞–≥–∞–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –±–∏—Ä–∂–∏ (–≤ –Ω–∞–Ω–æ—Å–µ–∫—É–Ω–¥–∞—Ö)
    """
    if config is None:
        config = {}
    
    logger.info("–û—Ü–µ–Ω–∫–∞ –ª–∞–≥–æ–≤ –º–µ–∂–¥—É –±–∏—Ä–∂–∞–º–∏...")
    logger.info("–≠—Ç–∞–ø 1/4: –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –ø–æ –±–∏—Ä–∂–∞–º")
    
    if trades_df.empty:
        logger.warning("Trades –¥–∞–Ω–Ω—ã–µ –ø—É—Å—Ç—ã, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω—É–ª–µ–≤—ã–µ –ª–∞–≥–∏")
        return {"binance": 0, "bybit": 0, "okx": 0}
    
    # 1. –ü–æ–ª—É—á–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –±–∏—Ä–∂–∏
    exchanges = trades_df['exchange'].unique()
    logger.info(f"–ù–∞–π–¥–µ–Ω—ã –±–∏—Ä–∂–∏: {exchanges}")
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    reference_exchange = config.get('reference_exchange', 'binance')
    max_lag_seconds = config.get('max_lag_seconds', 10)
    correlation_threshold = config.get('correlation_threshold', 0.7)
    time_bucket_ms = config.get('time_bucket_ms', 1000)
    min_data_points = config.get('min_data_points', 50)
    
    logger.info(f"–†–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–∞—è –±–∏—Ä–∂–∞: {reference_exchange}")
    logger.info(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ª–∞–≥: {max_lag_seconds} —Å–µ–∫")
    logger.info(f"–ü–æ—Ä–æ–≥ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏: {correlation_threshold}")
    
    # 2. –°–æ–∑–¥–∞–µ–º –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã —Ü–µ–Ω
    time_window_ns = time_bucket_ms * 1_000_000  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –Ω–∞–Ω–æ—Å–µ–∫—É–Ω–¥—ã
    logger.info("–≠—Ç–∞–ø 2/4: –°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ —Ü–µ–Ω")
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ –±–∏—Ä–∂–∞–º –∏ –≤—Ä–µ–º–µ–Ω–Ω—ã–º –æ–∫–Ω–∞–º
    price_series = {}
    
    for exchange in exchanges:
        ex_data = trades_df[trades_df['exchange'] == exchange].copy()
        if ex_data.empty:
            continue
            
        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫–Ω–∞ (–æ–∫—Ä—É–≥–ª—è–µ–º –¥–æ —Å–µ–∫—É–Ω–¥)
        ex_data['time_bucket'] = (ex_data['ts_ns'] // time_window_ns) * time_window_ns
        
        # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º —Å—Ä–µ–¥–Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω—É—é —Ü–µ–Ω—É –ø–æ –æ–±—ä–µ–º—É –≤ –∫–∞–∂–¥–æ–º –æ–∫–Ω–µ
        bucket_data = ex_data.groupby('time_bucket').agg({
            'price': 'mean',  # –°—Ä–µ–¥–Ω—è—è —Ü–µ–Ω–∞
            'size': 'sum'     # –û–±—â–∏–π –æ–±—ä–µ–º
        }).reset_index()
        
        price_series[exchange] = bucket_data.set_index('time_bucket')['price']
        logger.info(f"{exchange}: {len(bucket_data)} –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫–æ–Ω")
    
    # 3. –ù–∞—Ö–æ–¥–∏–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫–æ–Ω –º–µ–∂–¥—É –±–∏—Ä–∂–∞–º–∏
    if len(price_series) < 2:
        logger.warning("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±–∏—Ä–∂ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ª–∞–≥–æ–≤")
        return {ex: 0 for ex in exchanges}
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–∫–∞–∑–∞–Ω–Ω—É—é —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—É—é –±–∏—Ä–∂—É
    if reference_exchange not in price_series:
        reference_exchange = list(price_series.keys())[0]
        logger.warning(f"–†–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–∞—è –±–∏—Ä–∂–∞ {reference_exchange} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º {reference_exchange}")
    
    reference_series = price_series[reference_exchange]
    lags = {reference_exchange: 0}  # –†–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–∞—è –±–∏—Ä–∂–∞ –∏–º–µ–µ—Ç –ª–∞–≥ 0
    
    logger.info("–≠—Ç–∞–ø 3/4: –ê–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –º–µ–∂–¥—É –±–∏—Ä–∂–∞–º–∏")
    
    # 4. –î–ª—è –∫–∞–∂–¥–æ–π –±–∏—Ä–∂–∏ –≤—ã—á–∏—Å–ª—è–µ–º –∫—Ä–æ—Å—Å-–∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é —Å —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–æ–π
    for exchange in exchanges:
        if exchange == reference_exchange:
            continue
            
        target_series = price_series[exchange]
        
        # –ù–∞—Ö–æ–¥–∏–º –æ–±—â–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫–Ω–∞
        common_times = reference_series.index.intersection(target_series.index)
        
        if len(common_times) < min_data_points:  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            logger.warning(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –æ–±—â–∏—Ö —Ç–æ—á–µ–∫ –¥–ª—è {exchange}: {len(common_times)} < {min_data_points}")
            lags[exchange] = 0
            continue
        
        ref_prices = reference_series.loc[common_times]
        target_prices = target_series.loc[common_times]
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫—Ä–æ—Å—Å-–∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ª–∞–≥–∞
        # –ò—â–µ–º –ª–∞–≥ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ ¬±max_lag_seconds —Å–µ–∫—É–Ω–¥
        max_lag_buckets = max_lag_seconds
        correlations = []
        
        for lag in range(-max_lag_buckets, max_lag_buckets + 1):
            if lag == 0:
                corr = np.corrcoef(ref_prices, target_prices)[0, 1]
            elif lag > 0:
                # –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π –ª–∞–≥: target –æ—Ç—Å—Ç–∞–µ—Ç –æ—Ç reference
                if len(ref_prices) > lag:
                    corr = np.corrcoef(ref_prices[:-lag], target_prices[lag:])[0, 1]
                else:
                    corr = 0
            else:
                # –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π –ª–∞–≥: target –æ–ø–µ—Ä–µ–∂–∞–µ—Ç reference
                lag_abs = abs(lag)
                if len(target_prices) > lag_abs:
                    corr = np.corrcoef(ref_prices[lag_abs:], target_prices[:-lag_abs])[0, 1]
                else:
                    corr = 0
            
            if not np.isnan(corr):
                correlations.append((lag, corr))
        
        # –ù–∞—Ö–æ–¥–∏–º –ª–∞–≥ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–µ–π
        if correlations:
            best_lag, best_corr = max(correlations, key=lambda x: x[1])
            lag_ns = best_lag * time_window_ns  # –ü–µ—Ä–µ–≤–æ–¥–∏–º –≤ –Ω–∞–Ω–æ—Å–µ–∫—É–Ω–¥—ã
            
            logger.info(f"{exchange}: –ª–∞–≥ = {best_lag}—Å ({lag_ns}–Ω—Å), –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è = {best_corr:.3f}")
            lags[exchange] = lag_ns
        else:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã—á–∏—Å–ª–∏—Ç—å –ª–∞–≥ –¥–ª—è {exchange}")
            lags[exchange] = 0
    
    # 5. –í–∞–ª–∏–¥–∞—Ü–∏—è –ª–∞–≥–æ–≤ (–ø—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑—É–º–Ω–æ—Å—Ç—å)
    for exchange, lag in lags.items():
        if abs(lag) > 30_000_000_000:  # –ë–æ–ª–µ–µ 30 —Å–µ–∫—É–Ω–¥ - –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ
            logger.warning(f"–ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª—å—à–æ–π –ª–∞–≥ –¥–ª—è {exchange}: {lag/1e9:.1f}—Å, —Å–±—Ä–æ—Å –≤ 0")
            lags[exchange] = 0
    
    logger.info(f"–ò—Ç–æ–≥–æ–≤—ã–µ –ª–∞–≥–∏: {lags}")
    return lags

def sync_timestamps(
    trades_df: pd.DataFrame, 
    depth_df: pd.DataFrame,
    lags: Dict[str, int]
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–∞–π–º—Å—Ç–µ–º–ø—ã –º–µ–∂–¥—É –±–∏—Ä–∂–∞–º–∏
    
    Args:
        trades_df: DataFrame —Å trades
        depth_df: DataFrame —Å depth
        lags: –°–ª–æ–≤–∞—Ä—å –ª–∞–≥–æ–≤ –ø–æ –±–∏—Ä–∂–∞–º
        
    Returns:
        –ö–æ—Ä—Ç–µ–∂ (—Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ trades, —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ depth)
    """
    logger.info("–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è —Ç–∞–π–º—Å—Ç–µ–º–ø–æ–≤...")
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –ª–∞–≥–∏ –∫ trades –¥–∞–Ω–Ω—ã–º
    synced_trades = trades_df.copy() if not trades_df.empty else trades_df
    synced_depth = depth_df.copy() if not depth_df.empty else depth_df
    
    if not synced_trades.empty:
        logger.info("–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ª–∞–≥–æ–≤ –∫ trades –¥–∞–Ω–Ω—ã–º...")
        for exchange, lag_ns in lags.items():
            if lag_ns == 0:
                continue
                
            # –ù–∞—Ö–æ–¥–∏–º —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –¥–∞–Ω–Ω–æ–π –±–∏—Ä–∂–∏
            exchange_mask = synced_trades['exchange'] == exchange
            if exchange_mask.any():
                # –ü—Ä–∏–º–µ–Ω—è–µ–º –ª–∞–≥ (–≤—ã—á–∏—Ç–∞–µ–º –ª–∞–≥ –∏–∑ –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è –∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏–∏)
                synced_trades.loc[exchange_mask, 'ts_ns'] -= lag_ns
                logger.info(f"–ü—Ä–∏–º–µ–Ω–µ–Ω –ª–∞–≥ {lag_ns/1e6:.1f}–º—Å –∫ {exchange_mask.sum()} trades –æ—Ç {exchange}")
        
        # –ü–µ—Ä–µ—Å–æ—Ä—Ç–∏—Ä–æ–≤—ã–≤–∞–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –ø–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –ª–∞–≥–æ–≤
        synced_trades = synced_trades.sort_values('ts_ns').reset_index(drop=True)
        logger.info("Trades –¥–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É –≤—Ä–µ–º–µ–Ω–∏")
    
    if not synced_depth.empty:
        logger.info("–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ª–∞–≥–æ–≤ –∫ depth –¥–∞–Ω–Ω—ã–º...")
        for exchange, lag_ns in lags.items():
            if lag_ns == 0:
                continue
                
            # –ù–∞—Ö–æ–¥–∏–º —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –¥–∞–Ω–Ω–æ–π –±–∏—Ä–∂–∏
            exchange_mask = synced_depth['exchange'] == exchange
            if exchange_mask.any():
                # –ü—Ä–∏–º–µ–Ω—è–µ–º –ª–∞–≥ (–≤—ã—á–∏—Ç–∞–µ–º –ª–∞–≥ –∏–∑ –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è –∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏–∏)
                synced_depth.loc[exchange_mask, 'ts_ns'] -= lag_ns
                logger.info(f"–ü—Ä–∏–º–µ–Ω–µ–Ω –ª–∞–≥ {lag_ns/1e6:.1f}–º—Å –∫ {exchange_mask.sum()} depth –æ—Ç {exchange}")
        
        # –ü–µ—Ä–µ—Å–æ—Ä—Ç–∏—Ä–æ–≤—ã–≤–∞–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –ø–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –ª–∞–≥–æ–≤
        synced_depth = synced_depth.sort_values('ts_ns').reset_index(drop=True)
        logger.info("Depth –¥–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É –≤—Ä–µ–º–µ–Ω–∏")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏
    total_lag_applied = sum(abs(lag) for lag in lags.values() if lag != 0)
    if total_lag_applied > 0:
        logger.info(f"–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞, –ø—Ä–∏–º–µ–Ω–µ–Ω –æ–±—â–∏–π –ª–∞–≥ {total_lag_applied/1e6:.1f}–º—Å")
        
        # –õ–æ–≥–∏—Ä—É–µ–º –Ω–æ–≤—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–∏–∞–ø–∞–∑–æ–Ω—ã
        if not synced_trades.empty:
            logger.info(f"–ù–æ–≤—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω trades: {synced_trades.ts_ns.min()} - {synced_trades.ts_ns.max()}")
        if not synced_depth.empty:
            logger.info(f"–ù–æ–≤—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω depth: {synced_depth.ts_ns.min()} - {synced_depth.ts_ns.max()}")
    else:
        logger.info("–õ–∞–≥–∏ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã, —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è")
    
    return synced_trades, synced_depth

def filter_drops(
    trades_df: pd.DataFrame,
    depth_df: pd.DataFrame,
    min_overlap_ratio: float = 0.8
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –¥—Ä–æ–ø—ã –¥–∞–Ω–Ω—ã—Ö –∏ –Ω–µ–ø–æ–ª–Ω—ã–µ –ø–µ—Ä–∏–æ–¥—ã
    
    Args:
        trades_df: DataFrame —Å trades
        depth_df: DataFrame —Å depth
        min_overlap_ratio: –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –¥–∞–Ω–Ω—ã—Ö
        
    Returns:
        –ö–æ—Ä—Ç–µ–∂ (–æ—á–∏—â–µ–Ω–Ω—ã–µ trades, –æ—á–∏—â–µ–Ω–Ω—ã–µ depth)
    """
    logger.info("–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥—Ä–æ–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö...")
    
    filtered_trades = trades_df.copy() if not trades_df.empty else trades_df
    filtered_depth = depth_df.copy() if not depth_df.empty else depth_df
    
    if trades_df.empty and depth_df.empty:
        logger.warning("–û–±–∞ DataFrame –ø—É—Å—Ç—ã, —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è")
        return filtered_trades, filtered_depth
    
    # 1. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–±—â–∏–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω
    all_timestamps = []
    
    if not trades_df.empty:
        all_timestamps.extend(trades_df['ts_ns'].tolist())
    if not depth_df.empty:
        all_timestamps.extend(depth_df['ts_ns'].tolist())
    
    if not all_timestamps:
        logger.warning("–ù–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        return filtered_trades, filtered_depth
    
    global_start = min(all_timestamps)
    global_end = max(all_timestamps)
    total_duration = global_end - global_start
    
    logger.info(f"–û–±—â–∏–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω: {total_duration/1e9:.1f} —Å–µ–∫—É–Ω–¥")
    
    # 2. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ–∫—Ä—ã—Ç–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ –±–∏—Ä–∂–∞–º
    time_window_ns = 10_000_000_000  # 10 —Å–µ–∫—É–Ω–¥ - –æ–∫–Ω–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–æ–∫—Ä—ã—Ç–∏—è
    exchanges = set()
    
    if not trades_df.empty:
        exchanges.update(trades_df['exchange'].unique())
    if not depth_df.empty:
        exchanges.update(depth_df['exchange'].unique())
    
    logger.info(f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ–∫—Ä—ã—Ç–∏–µ –¥–ª—è –±–∏—Ä–∂: {exchanges}")
    
    # 3. –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é —Å–µ—Ç–∫—É –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–æ–∫—Ä—ã—Ç–∏—è
    time_buckets = np.arange(global_start, global_end + time_window_ns, time_window_ns)
    coverage_data = []
    
    for i in range(len(time_buckets) - 1):
        bucket_start = time_buckets[i]
        bucket_end = time_buckets[i + 1]
        
        # –°—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∏—Ä–∂ —Å –¥–∞–Ω–Ω—ã–º–∏ –≤ —ç—Ç–æ–º –æ–∫–Ω–µ
        exchanges_with_data = set()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º trades
        if not trades_df.empty:
            trades_in_bucket = trades_df[
                (trades_df['ts_ns'] >= bucket_start) & 
                (trades_df['ts_ns'] < bucket_end)
            ]
            if not trades_in_bucket.empty:
                exchanges_with_data.update(trades_in_bucket['exchange'].unique())
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º depth
        if not depth_df.empty:
            depth_in_bucket = depth_df[
                (depth_df['ts_ns'] >= bucket_start) & 
                (depth_df['ts_ns'] < bucket_end)
            ]
            if not depth_in_bucket.empty:
                exchanges_with_data.update(depth_in_bucket['exchange'].unique())
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ø–æ–∫—Ä—ã—Ç–∏—è
        coverage_ratio = len(exchanges_with_data) / len(exchanges) if exchanges else 0
        coverage_data.append({
            'start': bucket_start,
            'end': bucket_end,
            'coverage': coverage_ratio,
            'exchanges': exchanges_with_data
        })
    
    # 4. –ù–∞—Ö–æ–¥–∏–º –ø–µ—Ä–∏–æ–¥—ã —Å —Ö–æ—Ä–æ—à–∏–º –ø–æ–∫—Ä—ã—Ç–∏–µ–º
    good_periods = []
    current_period_start = None
    
    for bucket in coverage_data:
        if bucket['coverage'] >= min_overlap_ratio:
            if current_period_start is None:
                current_period_start = bucket['start']
        else:
            if current_period_start is not None:
                # –ó–∞–≤–µ—Ä—à–∞–µ–º —Ç–µ–∫—É—â–∏–π —Ö–æ—Ä–æ—à–∏–π –ø–µ—Ä–∏–æ–¥
                good_periods.append((current_period_start, bucket['start']))
                current_period_start = None
    
    # –ó–∞–∫—Ä—ã–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø–µ—Ä–∏–æ–¥ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if current_period_start is not None:
        good_periods.append((current_period_start, global_end))
    
    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(good_periods)} –ø–µ—Ä–∏–æ–¥–æ–≤ —Å –ø–æ–∫—Ä—ã—Ç–∏–µ–º >= {min_overlap_ratio:.1%}")
    
    # 5. –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ —Ö–æ—Ä–æ—à–∏–º –ø–µ—Ä–∏–æ–¥–∞–º
    if good_periods:
        # –°–æ–∑–¥–∞–µ–º –º–∞—Å–∫—É –¥–ª—è —Ö–æ—Ä–æ—à–∏—Ö –ø–µ—Ä–∏–æ–¥–æ–≤
        def is_in_good_period(timestamp):
            return any(start <= timestamp < end for start, end in good_periods)
        
        if not trades_df.empty:
            good_trades_mask = trades_df['ts_ns'].apply(is_in_good_period)
            filtered_trades = trades_df[good_trades_mask].reset_index(drop=True)
            
            removed_trades = len(trades_df) - len(filtered_trades)
            if removed_trades > 0:
                logger.info(f"–£–¥–∞–ª–µ–Ω–æ {removed_trades} trades ({removed_trades/len(trades_df)*100:.1f}%)")
            
        if not depth_df.empty:
            good_depth_mask = depth_df['ts_ns'].apply(is_in_good_period)
            filtered_depth = depth_df[good_depth_mask].reset_index(drop=True)
            
            removed_depth = len(depth_df) - len(filtered_depth)
            if removed_depth > 0:
                logger.info(f"–£–¥–∞–ª–µ–Ω–æ {removed_depth} depth records ({removed_depth/len(depth_df)*100:.1f}%)")
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∏—Ç–æ–≥–æ–≤–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ
        total_good_time = sum(end - start for start, end in good_periods)
        final_coverage = total_good_time / total_duration
        logger.info(f"–ò—Ç–æ–≥–æ–≤–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ –≤—Ä–µ–º–µ–Ω–∏: {final_coverage:.1%}")
        
    else:
        logger.warning("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–µ—Ä–∏–æ–¥–æ–≤ —Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º –ø–æ–∫—Ä—ã—Ç–∏–µ–º, –¥–∞–Ω–Ω—ã–µ –Ω–µ –∏–∑–º–µ–Ω–µ–Ω—ã")
    
    # 6. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è: —É–¥–∞–ª—è–µ–º –æ–¥–∏–Ω–æ—á–Ω—ã–µ —Ç–æ—á–∫–∏
    if not filtered_trades.empty:
        # –£–¥–∞–ª—è–µ–º trades, –∫–æ—Ç–æ—Ä—ã–µ —Å—Ç–æ—è—Ç –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ (–±–æ–ª—å—à–µ 60 —Å–µ–∫—É–Ω–¥ –æ—Ç –±–ª–∏–∂–∞–π—à–∏—Ö)
        isolation_threshold = 60_000_000_000  # 60 —Å–µ–∫—É–Ω–¥
        
        filtered_trades = filtered_trades.sort_values('ts_ns').reset_index(drop=True)
        to_keep = [True] * len(filtered_trades)
        
        for i in range(len(filtered_trades)):
            current_time = filtered_trades.iloc[i]['ts_ns']
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ –±–ª–∏–∂–∞–π—à–∏—Ö —Ç–æ—á–µ–∫
            prev_distance = float('inf')
            next_distance = float('inf')
            
            if i > 0:
                prev_distance = current_time - filtered_trades.iloc[i-1]['ts_ns']
            if i < len(filtered_trades) - 1:
                next_distance = filtered_trades.iloc[i+1]['ts_ns'] - current_time
            
            # –ï—Å–ª–∏ —Ç–æ—á–∫–∞ —Å–ª–∏—à–∫–æ–º –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–∞, –ø–æ–º–µ—á–∞–µ–º –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
            if min(prev_distance, next_distance) > isolation_threshold:
                to_keep[i] = False
        
        isolated_count = sum(not keep for keep in to_keep)
        if isolated_count > 0:
            filtered_trades = filtered_trades[to_keep].reset_index(drop=True)
            logger.info(f"–£–¥–∞–ª–µ–Ω–æ {isolated_count} –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö trades")
    
    logger.info("–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥—Ä–æ–ø–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    return filtered_trades, filtered_depth

def validate_sync_quality(
    trades_df: pd.DataFrame,
    depth_df: pd.DataFrame
) -> Dict[str, float]:
    """
    –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏
    
    Args:
        trades_df: –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ trades
        depth_df: –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ depth
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∫–∞—á–µ—Å—Ç–≤–∞
    """
    logger.info("–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏...")
    
    quality_metrics = {}
    
    # 1. –ê–Ω–∞–ª–∏–∑ –ø–æ–∫—Ä—ã—Ç–∏—è trades –¥–∞–Ω–Ω—ã—Ö
    if not trades_df.empty:
        exchanges = trades_df['exchange'].unique()
        
        # –í—ã—á–∏—Å–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ –ø–æ –±–∏—Ä–∂–∞–º
        total_time_range = trades_df['ts_ns'].max() - trades_df['ts_ns'].min()
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ –±–∏—Ä–∂–∞–º
        exchange_coverage = {}
        for exchange in exchanges:
            ex_data = trades_df[trades_df['exchange'] == exchange]
            ex_time_range = ex_data['ts_ns'].max() - ex_data['ts_ns'].min()
            coverage = ex_time_range / total_time_range if total_time_range > 0 else 0
            exchange_coverage[exchange] = coverage
        
        # –°—Ä–µ–¥–Ω—è—è –ø–æ–∫—Ä—ã—Ç–∏–µ –ø–æ –±–∏—Ä–∂–∞–º
        avg_trades_coverage = np.mean(list(exchange_coverage.values()))
        quality_metrics["trades_coverage"] = min(avg_trades_coverage, 1.0)
        
        # –†–∞–≤–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –æ—Ç —Ä–∞–≤–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è)
        expected_per_exchange = 1.0 / len(exchanges)
        actual_distribution = [len(trades_df[trades_df['exchange'] == ex]) / len(trades_df) 
                             for ex in exchanges]
        distribution_variance = np.var(actual_distribution)
        distribution_score = max(0, 1.0 - distribution_variance * 10)  # –®—Ç—Ä–∞—Ñ –∑–∞ –Ω–µ—Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç—å
        quality_metrics["trades_distribution"] = distribution_score
        
        logger.info(f"Trades –ø–æ–∫—Ä—ã—Ç–∏–µ –ø–æ –±–∏—Ä–∂–∞–º: {exchange_coverage}")
    else:
        quality_metrics["trades_coverage"] = 0.0
        quality_metrics["trades_distribution"] = 0.0
    
    # 2. –ê–Ω–∞–ª–∏–∑ –ø–æ–∫—Ä—ã—Ç–∏—è depth –¥–∞–Ω–Ω—ã—Ö
    if not depth_df.empty:
        exchanges = depth_df['exchange'].unique()
        
        total_time_range = depth_df['ts_ns'].max() - depth_df['ts_ns'].min()
        
        exchange_coverage = {}
        for exchange in exchanges:
            ex_data = depth_df[depth_df['exchange'] == exchange]
            ex_time_range = ex_data['ts_ns'].max() - ex_data['ts_ns'].min()
            coverage = ex_time_range / total_time_range if total_time_range > 0 else 0
            exchange_coverage[exchange] = coverage
        
        avg_depth_coverage = np.mean(list(exchange_coverage.values()))
        quality_metrics["depth_coverage"] = min(avg_depth_coverage, 1.0)
        
        logger.info(f"Depth –ø–æ–∫—Ä—ã—Ç–∏–µ –ø–æ –±–∏—Ä–∂–∞–º: {exchange_coverage}")
    else:
        quality_metrics["depth_coverage"] = 0.0
    
    # 3. –ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –º–µ–∂–¥—É –±–∏—Ä–∂–∞–º–∏
    if not trades_df.empty and len(trades_df['exchange'].unique()) > 1:
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ—Å—Ç—å –ø–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–º –æ–∫–Ω–∞–º
        window_size = 1_000_000_000  # 1 —Å–µ–∫—É–Ω–¥–∞
        
        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫–Ω–∞
        min_time = trades_df['ts_ns'].min()
        max_time = trades_df['ts_ns'].max()
        
        time_buckets = np.arange(min_time, max_time + window_size, window_size)
        alignment_scores = []
        
        for i in range(len(time_buckets) - 1):
            bucket_start = time_buckets[i]
            bucket_end = time_buckets[i + 1]
            
            # –î–∞–Ω–Ω—ã–µ –≤ —Ç–µ–∫—É—â–µ–º –æ–∫–Ω–µ
            window_data = trades_df[
                (trades_df['ts_ns'] >= bucket_start) & 
                (trades_df['ts_ns'] < bucket_end)
            ]
            
            if len(window_data) > 0:
                # –°—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∏—Ä–∂ –≤ –æ–∫–Ω–µ
                exchanges_in_window = len(window_data['exchange'].unique())
                total_exchanges = len(trades_df['exchange'].unique())
                
                # –°–∫–æ—Ä –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è = –¥–æ–ª—è –±–∏—Ä–∂ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –≤ –æ–∫–Ω–µ
                alignment_score = exchanges_in_window / total_exchanges
                alignment_scores.append(alignment_score)
        
        # –°—Ä–µ–¥–Ω–∏–π —Å–∫–æ—Ä –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è
        avg_alignment = np.mean(alignment_scores) if alignment_scores else 0
        quality_metrics["time_alignment"] = avg_alignment
        
        logger.info(f"–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(alignment_scores)} –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫–æ–Ω")
        logger.info(f"–°—Ä–µ–¥–Ω–∏–π —Å–∫–æ—Ä –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è: {avg_alignment:.3f}")
    else:
        quality_metrics["time_alignment"] = 1.0  # –û–¥–Ω–∞ –±–∏—Ä–∂–∞ –≤—Å–µ–≥–¥–∞ "–≤—ã—Ä–æ–≤–Ω–µ–Ω–∞"
    
    # 4. –ê–Ω–∞–ª–∏–∑ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö
    integrity_score = 1.0
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫ –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –±–∏—Ä–∂–∏
    if not trades_df.empty:
        for exchange in trades_df['exchange'].unique():
            ex_data = trades_df[trades_df['exchange'] == exchange]
            duplicates = ex_data['ts_ns'].duplicated().sum()
            if duplicates > 0:
                logger.warning(f"–ù–∞–π–¥–µ–Ω—ã –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è timestamps –≤ {exchange}: {duplicates}")
                integrity_score *= (1.0 - duplicates / len(ex_data))
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç—å –ø–æ—Å–ª–µ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
    if not trades_df.empty:
        is_sorted = trades_df['ts_ns'].is_monotonic_increasing
        if not is_sorted:
            logger.warning("–î–∞–Ω–Ω—ã–µ –Ω–µ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ –≤—Ä–µ–º–µ–Ω–∏")
            integrity_score *= 0.5
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ä–∞–∑—É–º–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã
    if not trades_df.empty and len(trades_df) > 1:
        time_diffs = trades_df['ts_ns'].diff().dropna()
        negative_diffs = (time_diffs < 0).sum()
        if negative_diffs > 0:
            logger.warning(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã: {negative_diffs}")
            integrity_score *= (1.0 - negative_diffs / len(time_diffs))
    
    quality_metrics["data_integrity"] = max(0.0, integrity_score)
    
    # 5. –û–±—â–∏–π —Å–∫–æ—Ä –∫–∞—á–µ—Å—Ç–≤–∞
    overall_score = np.mean(list(quality_metrics.values()))
    quality_metrics["overall_quality"] = overall_score
    
    # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    logger.info("=== –ú–ï–¢–†–ò–ö–ò –ö–ê–ß–ï–°–¢–í–ê –°–ò–ù–•–†–û–ù–ò–ó–ê–¶–ò–ò ===")
    for metric, score in quality_metrics.items():
        logger.info(f"{metric}: {score:.3f}")
    
    if overall_score >= 0.9:
        logger.info("üü¢ –ö–∞—á–µ—Å—Ç–≤–æ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏: –û–¢–õ–ò–ß–ù–û–ï")
    elif overall_score >= 0.7:
        logger.info("üü° –ö–∞—á–µ—Å—Ç–≤–æ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏: –•–û–†–û–®–ï–ï")
    elif overall_score >= 0.5:
        logger.info("üü† –ö–∞—á–µ—Å—Ç–≤–æ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏: –£–î–û–í–õ–ï–¢–í–û–†–ò–¢–ï–õ–¨–ù–û–ï")
    else:
        logger.warning("üî¥ –ö–∞—á–µ—Å—Ç–≤–æ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏: –ü–õ–û–•–û–ï")
    
    return quality_metrics

def run_time_sync(
    trades_df: pd.DataFrame,
    depth_df: pd.DataFrame,
    config: Dict
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –≤—Ä–µ–º–µ–Ω–∏
    
    Args:
        trades_df: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ trades
        depth_df: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ depth
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        
    Returns:
        –ö–æ—Ä—Ç–µ–∂ (—Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ trades, —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ depth)
    """
    logger.info("–ó–∞–ø—É—Å–∫ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –≤—Ä–µ–º–µ–Ω–∏...")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    sync_config = config.get('sync_time', {})
    log_progress = sync_config.get('log_progress', True)
    
    if log_progress:
        logger.info("–≠—Ç–∞–ø 1/4: –û—Ü–µ–Ω–∫–∞ –ª–∞–≥–æ–≤ –º–µ–∂–¥—É –±–∏—Ä–∂–∞–º–∏")
    
    # 1. –û—Ü–µ–Ω–∏—Ç—å –ª–∞–≥–∏
    lags = estimate_exchange_lags(trades_df, depth_df, sync_config)
    
    if log_progress:
        logger.info("–≠—Ç–∞–ø 2/4: –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫")
    
    # 2. –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–∞–π–º—Å—Ç–µ–º–ø—ã
    synced_trades, synced_depth = sync_timestamps(trades_df, depth_df, lags)
    
    if log_progress:
        logger.info("–≠—Ç–∞–ø 3/4: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥—Ä–æ–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö")
    
    # 3. –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –¥—Ä–æ–ø—ã
    filtered_trades, filtered_depth = filter_drops(
        synced_trades, synced_depth, 
        sync_config.get("min_data_coverage", 0.8)
    )
    
    if log_progress:
        logger.info("–≠—Ç–∞–ø 4/4: –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏")
    
    # 4. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ
    quality = validate_sync_quality(filtered_trades, filtered_depth)
    
    logger.info("–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    return filtered_trades, filtered_depth
