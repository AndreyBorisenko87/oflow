"""
–ë–ª–æ–∫ 10: –§–∏—á–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è 8 –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ (D1-D8)
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional
from pathlib import Path
import logging
import time

logger = logging.getLogger(__name__)

class DetectorFeatureExtractor:
    """–ò–∑–≤–ª–µ–∫–∞—Ç–µ–ª—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.tick_size_ms = config.get('time', {}).get('tick_size_ms', 10)
        self.top_levels = config.get('orderbook', {}).get('top_levels', 10)
        
        # –ü–æ—Ä–æ–≥–∏ –¥–ª—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤
        self.d1_vacuum_threshold = config.get('d1_vacuum', {}).get('threshold', 0.1)
        self.d1_empty_levels = config.get('d1_vacuum', {}).get('empty_levels', 3)
        
        self.d2_side_ratio = config.get('d2_absorption', {}).get('min_side_ratio', 0.7)
        self.d2_price_move = config.get('d2_absorption', {}).get('max_price_move_ticks', 2)
        
        self.d3_same_price_trades = config.get('d3_iceberg', {}).get('same_price_trades', 10)
        self.d3_time_window = config.get('d3_iceberg', {}).get('time_window_ms', 2000)
        
        self.d4_price_move = config.get('d4_stop_run', {}).get('min_price_move_percent', 0.3)
        self.d4_time_window = config.get('d4_stop_run', {}).get('time_window_ms', 2000)
        
        self.d5_reversal_threshold = config.get('d5_false_breakout', {}).get('reversal_threshold_ratio', 0.7)
        self.d5_reversal_window = config.get('d5_false_breakout', {}).get('reversal_window_ms', 5000)
        
        self.d6_bid_ask_ratio = config.get('d6_imbalance', {}).get('bid_ask_ratio', 3.0)
        self.d6_wall_multiplier = config.get('d6_imbalance', {}).get('level_wall_multiplier', 5.0)
        
        self.d7_large_order_multiplier = config.get('d7_spoofing', {}).get('large_order_multiplier', 5.0)
        self.d7_cancel_window = config.get('d7_spoofing', {}).get('cancel_time_window_ms', 1000)
        
        self.d8_acceleration_window = config.get('d8_momentum', {}).get('acceleration_window_ms', 3000)
        self.d8_volume_ratio = config.get('d8_momentum', {}).get('min_volume_ratio', 3.0)
        
    def run_feature_extraction(
        self,
        book_df: pd.DataFrame,
        trades_df: pd.DataFrame,
        quotes_df: pd.DataFrame
    ) -> pd.DataFrame:
        """–ü–æ–ª–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤"""
        start_time = time.time()
        logger.info("=== –ù–∞—á–∞–ª–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤ ===")
        
        # –≠—Ç–∞–ø 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        logger.info("–≠—Ç–∞–ø 1/6: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...")
        validated_data = self._validate_and_prepare_data(book_df, trades_df, quotes_df)
        
        # –≠—Ç–∞–ø 2: –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ Order Flow
        logger.info("–≠—Ç–∞–ø 2/6: –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ Order Flow...")
        features_df = self._extract_order_flow_features(validated_data)
        
        # –≠—Ç–∞–ø 3: –ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è D1 (–í–∞–∫—É—É–º –ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç–∏)
        logger.info("–≠—Ç–∞–ø 3/6: –ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è D1 (–í–∞–∫—É—É–º –ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç–∏)...")
        features_df = self._extract_d1_vacuum_features(features_df, validated_data)
        
        # –≠—Ç–∞–ø 4: –ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è D2-D4 (–ü–æ–≥–ª–æ—â–µ–Ω–∏–µ, –ê–π—Å–±–µ—Ä–≥, –°—Ç–æ–ø-—Ä–∞–Ω)
        logger.info("–≠—Ç–∞–ø 4/6: –ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è D2-D4 (–ü–æ–≥–ª–æ—â–µ–Ω–∏–µ, –ê–π—Å–±–µ—Ä–≥, –°—Ç–æ–ø-—Ä–∞–Ω)...")
        features_df = self._extract_d2_d4_features(features_df, validated_data)
        
        # –≠—Ç–∞–ø 5: –ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è D5-D7 (–õ–æ–∂–Ω—ã–π –≤—ã–Ω–æ—Å, –ò–º–±–∞–ª–∞–Ω—Å, –°–ø—É—Ñ–∏–Ω–≥)
        logger.info("–≠—Ç–∞–ø 5/6: –ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è D5-D7 (–õ–æ–∂–Ω—ã–π –≤—ã–Ω–æ—Å, –ò–º–±–∞–ª–∞–Ω—Å, –°–ø—É—Ñ–∏–Ω–≥)...")
        features_df = self._extract_d5_d7_features(features_df, validated_data)
        
        # –≠—Ç–∞–ø 6: –ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è D8 (–ò–º–ø—É–ª—å—Å)
        logger.info("–≠—Ç–∞–ø 6/6: –ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è D8 (–ò–º–ø—É–ª—å—Å)...")
        features_df = self._extract_d8_momentum_features(features_df, validated_data)
        
        # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ
        duration = time.time() - start_time
        logger.info(f"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {duration:.2f}—Å")
        logger.info(f"–ü—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑–≤–ª–µ—á–µ–Ω—ã: {len(features_df)} –∑–∞–ø–∏—Å–µ–π")
        logger.info("=== –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–æ ===")
        
        return features_df
    
    def _validate_and_prepare_data(
        self,
        book_df: pd.DataFrame,
        trades_df: pd.DataFrame,
        quotes_df: pd.DataFrame
    ) -> Dict[str, pd.DataFrame]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
        logger.info("–í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        validated_data = {}
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è book_df
        if not book_df.empty:
            required_book_columns = ['ts_ns', 'exchange', 'symbol', 'side', 'price', 'size', 'action']
            missing_book = [col for col in required_book_columns if col not in book_df.columns]
            
            if not missing_book:
                book_df = book_df.copy()
                book_df = book_df.sort_values('ts_ns').reset_index(drop=True)
                book_df['ts_group'] = (book_df['ts_ns'] // (self.tick_size_ms * 1_000_000)).astype(int)
                validated_data['book'] = book_df
                logger.info(f"Book –¥–∞–Ω–Ω—ã–µ –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω—ã: {len(book_df)} –∑–∞–ø–∏—Å–µ–π")
            else:
                logger.warning(f"Book –¥–∞–Ω–Ω—ã–µ –Ω–µ –ø—Ä–æ—à–ª–∏ –≤–∞–ª–∏–¥–∞—Ü–∏—é: –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç {missing_book}")
                validated_data['book'] = pd.DataFrame()
        else:
            validated_data['book'] = pd.DataFrame()
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è trades_df
        if not trades_df.empty:
            required_trades_columns = ['ts_ns', 'exchange', 'symbol', 'price', 'size', 'side']
            missing_trades = [col for col in required_trades_columns if col not in trades_df.columns]
            
            if not missing_trades:
                trades_df = trades_df.copy()
                trades_df = trades_df.sort_values('ts_ns').reset_index(drop=True)
                trades_df['ts_group'] = (trades_df['ts_ns'] // (self.tick_size_ms * 1_000_000)).astype(int)
                validated_data['trades'] = trades_df
                logger.info(f"Trades –¥–∞–Ω–Ω—ã–µ –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω—ã: {len(trades_df)} –∑–∞–ø–∏—Å–µ–π")
            else:
                logger.warning(f"Trades –¥–∞–Ω–Ω—ã–µ –Ω–µ –ø—Ä–æ—à–ª–∏ –≤–∞–ª–∏–¥–∞—Ü–∏—é: –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç {missing_trades}")
                validated_data['trades'] = pd.DataFrame()
        else:
            validated_data['trades'] = pd.DataFrame()
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è quotes_df
        if not quotes_df.empty:
            required_quotes_columns = ['ts_ns', 'exchange', 'symbol', 'bid', 'ask', 'bid_size', 'ask_size']
            missing_quotes = [col for col in required_quotes_columns if col not in quotes_df.columns]
            
            if not missing_quotes:
                quotes_df = quotes_df.copy()
                quotes_df = quotes_df.sort_values('ts_ns').reset_index(drop=True)
                quotes_df['ts_group'] = (quotes_df['ts_ns'] // (self.tick_size_ms * 1_000_000)).astype(int)
                validated_data['quotes'] = quotes_df
                logger.info(f"Quotes –¥–∞–Ω–Ω—ã–µ –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω—ã: {len(quotes_df)} –∑–∞–ø–∏—Å–µ–π")
            else:
                logger.warning(f"Quotes –¥–∞–Ω–Ω—ã–µ –Ω–µ –ø—Ä–æ—à–ª–∏ –≤–∞–ª–∏–¥–∞—Ü–∏—é: –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç {missing_quotes}")
                validated_data['quotes'] = pd.DataFrame()
        else:
            validated_data['quotes'] = pd.DataFrame()
        
        return validated_data
    
    def _extract_order_flow_features(self, validated_data: Dict[str, pd.DataFrame]) -> pd.DataFrame:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö Order Flow –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        logger.info("–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö Order Flow –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        
        if validated_data['book'].empty:
            logger.warning("Book –¥–∞–Ω–Ω—ã–µ –ø—É—Å—Ç—ã, —Å–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π DataFrame")
            return pd.DataFrame(columns=['ts_ns', 'ts_group', 'exchange', 'symbol'])
        
        book_df = validated_data['book']
        
        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–º –≥—Ä—É–ø–ø–∞–º
        order_flow_features = []
        total_groups = book_df['ts_group'].nunique()
        
        for i, (ts_group, group_data) in enumerate(book_df.groupby('ts_group')):
            if i % 1000 == 0:  # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–µ 1000 –≥—Ä—É–ø–ø
                logger.info(f"–ü—Ä–æ–≥—Ä–µ—Å—Å order flow: {i+1}/{total_groups} –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –≥—Ä—É–ø–ø")
            
            # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ –±–∏—Ä–∂–∞–º –∏ —Å–∏–º–≤–æ–ª–∞–º
            for (exchange, symbol), exchange_data in group_data.groupby(['exchange', 'symbol']):
                # –ü–æ–¥—Å—á–µ—Ç –¥–µ–π—Å—Ç–≤–∏–π
                cancellations = len(exchange_data[exchange_data['action'] == 'delete'])
                additions = len(exchange_data[exchange_data['action'] == 'add'])
                updates = len(exchange_data[exchange_data['action'] == 'update'])
                total_actions = len(exchange_data)
                
                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å—Ç–æ—Ä–æ–Ω–∞–º
                bid_cancellations = len(exchange_data[
                    (exchange_data['action'] == 'delete') & 
                    (exchange_data['side'] == 'bid')
                ])
                ask_cancellations = len(exchange_data[
                    (exchange_data['action'] == 'delete') & 
                    (exchange_data['side'] == 'ask')
                ])
                
                bid_additions = len(exchange_data[
                    (exchange_data['action'] == 'add') & 
                    (exchange_data['side'] == 'bid')
                ])
                ask_additions = len(exchange_data[
                    (exchange_data['action'] == 'add') & 
                    (exchange_data['side'] == 'ask')
                ])
                
                # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                cancellation_rate = cancellations / total_actions if total_actions > 0 else 0
                addition_rate = additions / total_actions if total_actions > 0 else 0
                update_rate = updates / total_actions if total_actions > 0 else 0
                
                bid_ask_imbalance = (bid_additions - ask_additions) / (bid_additions + ask_additions + 1e-8)
                cancellation_imbalance = (bid_cancellations - ask_cancellations) / (bid_cancellations + ask_cancellations + 1e-8)
                
                # –û–±—ä–µ–º–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
                total_volume = exchange_data['size'].sum()
                avg_order_size = exchange_data['size'].mean()
                volume_std = exchange_data['size'].std()
                
                order_flow_features.append({
                    'ts_ns': exchange_data['ts_ns'].iloc[0],
                    'ts_group': ts_group,
                    'exchange': exchange,
                    'symbol': symbol,
                    'cancellation_rate': cancellation_rate,
                    'addition_rate': addition_rate,
                    'update_rate': update_rate,
                    'bid_ask_imbalance': bid_ask_imbalance,
                    'cancellation_imbalance': cancellation_imbalance,
                    'total_volume': total_volume,
                    'avg_order_size': avg_order_size,
                    'volume_std': volume_std,
                    'total_actions': total_actions,
                    'cancellations': cancellations,
                    'additions': additions,
                    'updates': updates
                })
        
        features_df = pd.DataFrame(order_flow_features)
        
        if not features_df.empty:
            logger.info(f"Order flow –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑–≤–ª–µ—á–µ–Ω—ã: {len(features_df)} –∑–∞–ø–∏—Å–µ–π")
        else:
            logger.warning("Order flow –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–µ –∏–∑–≤–ª–µ—á–µ–Ω—ã")
        
        return features_df
    
    def _extract_d1_vacuum_features(
        self,
        features_df: pd.DataFrame,
        validated_data: Dict[str, pd.DataFrame]
    ) -> pd.DataFrame:
        """–ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è D1: –í–∞–∫—É—É–º –ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç–∏"""
        logger.info("–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è D1 (–í–∞–∫—É—É–º –ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç–∏)...")
        
        if validated_data['quotes'].empty:
            logger.warning("Quotes –¥–∞–Ω–Ω—ã–µ –ø—É—Å—Ç—ã, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º D1")
            features_df['d1_vacuum_score'] = 0.0
            features_df['d1_empty_levels'] = 0
            features_df['d1_liquidity_depth'] = 0.0
            return features_df
        
        quotes_df = validated_data['quotes']
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –≥—Ä—É–ø–ø –¥–ª—è features_df –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
        if 'ts_group' not in features_df.columns:
            features_df['ts_group'] = (features_df['ts_ns'] // (self.tick_size_ms * 1_000_000)).astype(int)
        
        # –ê–Ω–∞–ª–∏–∑ –≤–∞–∫—É—É–º–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–º –≥—Ä—É–ø–ø–∞–º
        vacuum_features = []
        
        for ts_group, group_data in features_df.groupby('ts_group'):
            # –ü–æ–∏—Å–∫ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö quotes –¥–∞–Ω–Ω—ã—Ö
            quotes_group = quotes_df[quotes_df['ts_group'] == ts_group]
            
            if quotes_group.empty:
                continue
            
            for _, feature_row in group_data.iterrows():
                exchange = feature_row['exchange']
                symbol = feature_row['symbol']
                
                # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è quotes –ø–æ –±–∏—Ä–∂–µ –∏ —Å–∏–º–≤–æ–ª—É
                exchange_quotes = quotes_group[
                    (quotes_group['exchange'] == exchange) & 
                    (quotes_group['symbol'] == symbol)
                ]
                
                if exchange_quotes.empty:
                    vacuum_features.append({
                        'ts_group': ts_group,
                        'exchange': exchange,
                        'symbol': symbol,
                        'd1_vacuum_score': 0.0,
                        'd1_empty_levels': 0,
                        'd1_liquidity_depth': 0.0
                    })
                    continue
                
                # –ê–Ω–∞–ª–∏–∑ –≥–ª—É–±–∏–Ω—ã –ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç–∏
                bid_depth = exchange_quotes['bid_size'].sum()
                ask_depth = exchange_quotes['ask_size'].sum()
                total_depth = bid_depth + ask_depth
                
                # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—É—Å—Ç—ã—Ö —É—Ä–æ–≤–Ω–µ–π
                empty_bid_levels = len(exchange_quotes[exchange_quotes['bid_size'] < self.d1_vacuum_threshold])
                empty_ask_levels = len(exchange_quotes[exchange_quotes['ask_size'] < self.d1_vacuum_threshold])
                total_empty_levels = empty_bid_levels + empty_ask_levels
                
                # –°–∫–æ—Ä–∏–Ω–≥ –≤–∞–∫—É—É–º–∞ –¥–ª—è D1
                d1_vacuum_score = 0.0
                if total_empty_levels >= self.d1_empty_levels:
                    # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Å–∫–æ—Ä–∏–Ω–≥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—É—Å—Ç—ã—Ö —É—Ä–æ–≤–Ω–µ–π –∏ –≥–ª—É–±–∏–Ω—ã
                    empty_ratio = total_empty_levels / (len(exchange_quotes) * 2)  # bid + ask
                    depth_ratio = 1.0 / (1.0 + total_depth / 1000)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≥–ª—É–±–∏–Ω—ã
                    d1_vacuum_score = (empty_ratio * 0.7 + depth_ratio * 0.3)
                
                vacuum_features.append({
                    'ts_group': ts_group,
                    'exchange': exchange,
                    'symbol': symbol,
                    'd1_vacuum_score': d1_vacuum_score,
                    'd1_empty_levels': total_empty_levels,
                    'd1_liquidity_depth': total_depth
                })
        
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –æ—Å–Ω–æ–≤–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        vacuum_df = pd.DataFrame(vacuum_features)
        if not vacuum_df.empty:
            features_df = features_df.merge(
                vacuum_df[['ts_group', 'exchange', 'symbol', 'd1_vacuum_score', 'd1_empty_levels', 'd1_liquidity_depth']],
                on=['ts_group', 'exchange', 'symbol'],
                how='left'
            )
            
            # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
            features_df['d1_vacuum_score'] = features_df['d1_vacuum_score'].fillna(0.0)
            features_df['d1_empty_levels'] = features_df['d1_empty_levels'].fillna(0)
            features_df['d1_liquidity_depth'] = features_df['d1_liquidity_depth'].fillna(0.0)
            
            logger.info("–ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è D1 (–í–∞–∫—É—É–º –ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç–∏) –¥–æ–±–∞–≤–ª–µ–Ω—ã")
        else:
            # –ï—Å–ª–∏ –≤–∞–∫—É—É–º –Ω–µ –∏–∑–≤–ª–µ—á–µ–Ω, –¥–æ–±–∞–≤–ª—è–µ–º –ø—É—Å—Ç—ã–µ –∫–æ–ª–æ–Ω–∫–∏
            features_df['d1_vacuum_score'] = 0.0
            features_df['d1_empty_levels'] = 0
            features_df['d1_liquidity_depth'] = 0.0
        
        return features_df
    
    def _extract_d2_d4_features(
        self,
        features_df: pd.DataFrame,
        validated_data: Dict[str, pd.DataFrame]
    ) -> pd.DataFrame:
        """–ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è D2-D4: –ü–æ–≥–ª–æ—â–µ–Ω–∏–µ, –ê–π—Å–±–µ—Ä–≥, –°—Ç–æ–ø-—Ä–∞–Ω"""
        logger.info("–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è D2-D4...")
        
        if validated_data['trades'].empty:
            logger.warning("Trades –¥–∞–Ω–Ω—ã–µ –ø—É—Å—Ç—ã, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º D2-D4")
            # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Å—Ç—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è D2-D4
            features_df['d2_side_ratio'] = 0.0
            features_df['d2_price_stable'] = False
            features_df['d3_same_price_count'] = 0
            features_df['d3_volume_concentration'] = 0.0
            features_df['d4_price_momentum'] = 0.0
            features_df['d4_volume_surge'] = 0.0
            return features_df
        
        trades_df = validated_data['trades']
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–º –≥—Ä—É–ø–ø–∞–º
        d2_d4_features = []
        
        for ts_group, group_data in features_df.groupby('ts_group'):
            # –ü–æ–∏—Å–∫ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö trades –¥–∞–Ω–Ω—ã—Ö
            trades_group = trades_df[trades_df['ts_group'] == ts_group]
            
            if trades_group.empty:
                continue
            
            for _, feature_row in group_data.iterrows():
                exchange = feature_row['exchange']
                symbol = feature_row['symbol']
                
                # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è trades –ø–æ –±–∏—Ä–∂–µ –∏ —Å–∏–º–≤–æ–ª—É
                exchange_trades = trades_group[
                    (trades_group['exchange'] == exchange) & 
                    (trades_group['symbol'] == symbol)
                ]
                
                if exchange_trades.empty:
                    d2_d4_features.append({
                        'ts_group': ts_group,
                        'exchange': exchange,
                        'symbol': symbol,
                        'd2_side_ratio': 0.0,
                        'd2_price_stable': False,
                        'd3_same_price_count': 0,
                        'd3_volume_concentration': 0.0,
                        'd4_price_momentum': 0.0,
                        'd4_volume_surge': 0.0
                    })
                    continue
                
                # D2: –ü—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ–≥–ª–æ—â–µ–Ω–∏—è
                buy_trades = len(exchange_trades[exchange_trades['side'] == 'buy'])
                sell_trades = len(exchange_trades[exchange_trades['side'] == 'sell'])
                total_trades = len(exchange_trades)
                
                d2_side_ratio = max(buy_trades, sell_trades) / total_trades if total_trades > 0 else 0
                d2_price_stable = len(exchange_trades['price'].unique()) <= self.d2_price_move
                
                # D3: –ü—Ä–∏–∑–Ω–∞–∫–∏ –∞–π—Å–±–µ—Ä–≥–∞
                price_counts = exchange_trades['price'].value_counts()
                d3_same_price_count = price_counts.max() if not price_counts.empty else 0
                d3_volume_concentration = d3_same_price_count / total_trades if total_trades > 0 else 0
                
                # D4: –ü—Ä–∏–∑–Ω–∞–∫–∏ —Å—Ç–æ–ø-—Ä–∞–Ω–∞
                if len(exchange_trades) > 1:
                    price_changes = exchange_trades['price'].pct_change().abs()
                    d4_price_momentum = price_changes.mean()
                    
                    volume_surge = exchange_trades['size'].sum() / (self.d4_time_window / 1000) if self.d4_time_window > 0 else 0
                    d4_volume_surge = min(volume_surge / 1000, 1.0)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
                else:
                    d4_price_momentum = 0.0
                    d4_volume_surge = 0.0
                
                d2_d4_features.append({
                    'ts_group': ts_group,
                    'exchange': exchange,
                    'symbol': symbol,
                    'd2_side_ratio': d2_side_ratio,
                    'd2_price_stable': d2_price_stable,
                    'd3_same_price_count': d3_same_price_count,
                    'd3_volume_concentration': d3_volume_concentration,
                    'd4_price_momentum': d4_price_momentum,
                    'd4_volume_surge': d4_volume_surge
                })
        
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –æ—Å–Ω–æ–≤–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        d2_d4_df = pd.DataFrame(d2_d4_features)
        if not d2_d4_df.empty:
            features_df = features_df.merge(
                d2_d4_df[['ts_group', 'exchange', 'symbol', 'd2_side_ratio', 'd2_price_stable', 
                          'd3_same_price_count', 'd3_volume_concentration', 'd4_price_momentum', 'd4_volume_surge']],
                on=['ts_group', 'exchange', 'symbol'],
                how='left'
            )
            
            # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
            for col in ['d2_side_ratio', 'd3_volume_concentration', 'd4_price_momentum', 'd4_volume_surge']:
                features_df[col] = features_df[col].fillna(0.0)
            features_df['d2_price_stable'] = features_df['d2_price_stable'].fillna(False)
            features_df['d3_same_price_count'] = features_df['d3_same_price_count'].fillna(0)
            
            logger.info("–ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è D2-D4 –¥–æ–±–∞–≤–ª–µ–Ω—ã")
        else:
            # –ï—Å–ª–∏ –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–µ –∏–∑–≤–ª–µ—á–µ–Ω—ã, –¥–æ–±–∞–≤–ª—è–µ–º –ø—É—Å—Ç—ã–µ –∫–æ–ª–æ–Ω–∫–∏
            features_df['d2_side_ratio'] = 0.0
            features_df['d2_price_stable'] = False
            features_df['d3_same_price_count'] = 0
            features_df['d3_volume_concentration'] = 0.0
            features_df['d4_price_momentum'] = 0.0
            features_df['d4_volume_surge'] = 0.0
        
        return features_df
    
    def _extract_d5_d7_features(
        self,
        features_df: pd.DataFrame,
        validated_data: Dict[str, pd.DataFrame]
    ) -> pd.DataFrame:
        """–ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è D5-D7: –õ–æ–∂–Ω—ã–π –≤—ã–Ω–æ—Å, –ò–º–±–∞–ª–∞–Ω—Å, –°–ø—É—Ñ–∏–Ω–≥"""
        logger.info("–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è D5-D7...")
        
        if validated_data['book'].empty:
            logger.warning("Book –¥–∞–Ω–Ω—ã–µ –ø—É—Å—Ç—ã, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º D5-D7")
            # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Å—Ç—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è D5-D7
            features_df['d5_reversal_probability'] = 0.0
            features_df['d6_bid_ask_imbalance'] = 0.0
            features_df['d6_level_wall'] = 0.0
            features_df['d7_spoofing_score'] = 0.0
            features_df['d7_quick_cancel'] = False
            return features_df
        
        book_df = validated_data['book']
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–º –≥—Ä—É–ø–ø–∞–º
        d5_d7_features = []
        
        for ts_group, group_data in features_df.groupby('ts_group'):
            # –ü–æ–∏—Å–∫ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö book –¥–∞–Ω–Ω—ã—Ö
            book_group = book_df[book_df['ts_group'] == ts_group]
            
            if book_group.empty:
                continue
            
            for _, feature_row in group_data.iterrows():
                exchange = feature_row['exchange']
                symbol = feature_row['symbol']
                
                # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è book –ø–æ –±–∏—Ä–∂–µ –∏ —Å–∏–º–≤–æ–ª—É
                exchange_book = book_group[
                    (book_group['exchange'] == exchange) & 
                    (book_group['symbol'] == symbol)
                ]
                
                if exchange_book.empty:
                    d5_d7_features.append({
                        'ts_group': ts_group,
                        'exchange': exchange,
                        'symbol': symbol,
                        'd5_reversal_probability': 0.0,
                        'd6_bid_ask_imbalance': 0.0,
                        'd6_level_wall': 0.0,
                        'd7_spoofing_score': 0.0,
                        'd7_quick_cancel': False
                    })
                    continue
                
                # D5: –ü—Ä–∏–∑–Ω–∞–∫–∏ –ª–æ–∂–Ω–æ–≥–æ –≤—ã–Ω–æ—Å–∞ (—É–ø—Ä–æ—â–µ–Ω–Ω–æ)
                d5_reversal_probability = 0.0
                if len(exchange_book) > 1:
                    # –ê–Ω–∞–ª–∏–∑ –æ—Ç–º–µ–Ω vs –¥–æ–±–∞–≤–ª–µ–Ω–∏–π
                    cancellations = len(exchange_book[exchange_book['action'] == 'delete'])
                    additions = len(exchange_book[exchange_book['action'] == 'add'])
                    total_actions = len(exchange_book)
                    
                    if total_actions > 0:
                        d5_reversal_probability = cancellations / total_actions
                
                # D6: –ü—Ä–∏–∑–Ω–∞–∫–∏ –∏–º–±–∞–ª–∞–Ω—Å–∞ —Å—Ç–∞–∫–∞–Ω–∞
                bid_volume = exchange_book[exchange_book['side'] == 'bid']['size'].sum()
                ask_volume = exchange_book[exchange_book['side'] == 'ask']['size'].sum()
                
                d6_bid_ask_imbalance = 0.0
                if ask_volume > 0:
                    d6_bid_ask_imbalance = bid_volume / ask_volume
                
                # –ê–Ω–∞–ª–∏–∑ —Å—Ç–µ–Ω –Ω–∞ —É—Ä–æ–≤–Ω—è—Ö
                level_volumes = exchange_book.groupby(['side', 'price'])['size'].sum()
                d6_level_wall = level_volumes.max() / (level_volumes.mean() + 1e-8) if not level_volumes.empty else 0
                
                # D7: –ü—Ä–∏–∑–Ω–∞–∫–∏ —Å–ø—É—Ñ–∏–Ω–≥–∞
                d7_spoofing_score = 0.0
                d7_quick_cancel = False
                
                if len(exchange_book) > 1:
                    # –ê–Ω–∞–ª–∏–∑ –±—ã—Å—Ç—Ä—ã—Ö –æ—Ç–º–µ–Ω
                    add_actions = exchange_book[exchange_book['action'] == 'add']
                    if not add_actions.empty:
                        for _, add_row in add_actions.iterrows():
                            # –ü–æ–∏—Å–∫ –±—ã—Å—Ç—Ä—ã—Ö –æ—Ç–º–µ–Ω
                            cancel_actions = exchange_book[
                                (exchange_book['action'] == 'delete') &
                                (exchange_book['side'] == add_row['side']) &
                                (exchange_book['price'] == add_row['price'])
                            ]
                            
                            if not cancel_actions.empty:
                                time_diff = cancel_actions['ts_ns'].iloc[0] - add_row['ts_ns']
                                if time_diff <= self.d7_cancel_window * 1_000_000:  # –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ –Ω–∞–Ω–æ—Å–µ–∫—É–Ω–¥—ã
                                    d7_quick_cancel = True
                                    d7_spoofing_score = 1.0
                                    break
                
                d5_d7_features.append({
                    'ts_group': ts_group,
                    'exchange': exchange,
                    'symbol': symbol,
                    'd5_reversal_probability': d5_reversal_probability,
                    'd6_bid_ask_imbalance': d6_bid_ask_imbalance,
                    'd6_level_wall': d6_level_wall,
                    'd7_spoofing_score': d7_spoofing_score,
                    'd7_quick_cancel': d7_quick_cancel
                })
        
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –æ—Å–Ω–æ–≤–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        d5_d7_df = pd.DataFrame(d5_d7_features)
        if not d5_d7_df.empty:
            features_df = features_df.merge(
                d5_d7_df[['ts_group', 'exchange', 'symbol', 'd5_reversal_probability', 'd6_bid_ask_imbalance', 
                          'd6_level_wall', 'd7_spoofing_score', 'd7_quick_cancel']],
                on=['ts_group', 'exchange', 'symbol'],
                how='left'
            )
            
            # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
            features_df['d5_reversal_probability'] = features_df['d5_reversal_probability'].fillna(0.0)
            features_df['d6_bid_ask_imbalance'] = features_df['d6_bid_ask_imbalance'].fillna(0.0)
            features_df['d6_level_wall'] = features_df['d6_level_wall'].fillna(0.0)
            features_df['d7_spoofing_score'] = features_df['d7_spoofing_score'].fillna(0.0)
            features_df['d7_quick_cancel'] = features_df['d7_quick_cancel'].fillna(False)
            
            logger.info("–ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è D5-D7 –¥–æ–±–∞–≤–ª–µ–Ω—ã")
        else:
            # –ï—Å–ª–∏ –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–µ –∏–∑–≤–ª–µ—á–µ–Ω—ã, –¥–æ–±–∞–≤–ª—è–µ–º –ø—É—Å—Ç—ã–µ –∫–æ–ª–æ–Ω–∫–∏
            features_df['d5_reversal_probability'] = 0.0
            features_df['d6_bid_ask_imbalance'] = 0.0
            features_df['d6_level_wall'] = 0.0
            features_df['d7_spoofing_score'] = 0.0
            features_df['d7_quick_cancel'] = False
        
        return features_df
    
    def _extract_d8_momentum_features(
        self,
        features_df: pd.DataFrame,
        validated_data: Dict[str, pd.DataFrame]
    ) -> pd.DataFrame:
        """–ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è D8: –ò–º–ø—É–ª—å—Å"""
        logger.info("–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è D8 (–ò–º–ø—É–ª—å—Å)...")
        
        if validated_data['trades'].empty:
            logger.warning("Trades –¥–∞–Ω–Ω—ã–µ –ø—É—Å—Ç—ã, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º D8")
            features_df['d8_momentum_score'] = 0.0
            features_df['d8_acceleration'] = 0.0
            features_df['d8_volume_surge'] = 0.0
            return features_df
        
        trades_df = validated_data['trades']
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–º –≥—Ä—É–ø–ø–∞–º
        momentum_features = []
        
        for ts_group, group_data in features_df.groupby('ts_group'):
            # –ü–æ–∏—Å–∫ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö trades –¥–∞–Ω–Ω—ã—Ö
            trades_group = trades_df[trades_df['ts_group'] == ts_group]
            
            if trades_group.empty:
                continue
            
            for _, feature_row in group_data.iterrows():
                exchange = feature_row['exchange']
                symbol = feature_row['symbol']
                
                # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è trades –ø–æ –±–∏—Ä–∂–µ –∏ —Å–∏–º–≤–æ–ª—É
                exchange_trades = trades_group[
                    (trades_group['exchange'] == exchange) & 
                    (trades_group['symbol'] == symbol)
                ]
                
                if exchange_trades.empty:
                    momentum_features.append({
                        'ts_group': ts_group,
                        'exchange': exchange,
                        'symbol': symbol,
                        'd8_momentum_score': 0.0,
                        'd8_acceleration': 0.0,
                        'd8_volume_surge': 0.0
                    })
                    continue
                
                # D8: –ü—Ä–∏–∑–Ω–∞–∫–∏ –∏–º–ø—É–ª—å—Å–∞
                d8_momentum_score = 0.0
                d8_acceleration = 0.0
                d8_volume_surge = 0.0
                
                if len(exchange_trades) > 1:
                    # –ê–Ω–∞–ª–∏–∑ —É—Å–∫–æ—Ä–µ–Ω–∏—è —Ü–µ–Ω—ã
                    price_changes = exchange_trades['price'].pct_change()
                    if len(price_changes) > 1:
                        d8_acceleration = price_changes.diff().mean()
                    
                    # –ê–Ω–∞–ª–∏–∑ –≤—Å–ø–ª–µ—Å–∫–∞ –æ–±—ä–µ–º–∞
                    volume_per_second = exchange_trades['size'].sum() / (self.d8_acceleration_window / 1000) if self.d8_acceleration_window > 0 else 0
                    d8_volume_surge = min(volume_per_second / 1000, 1.0)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
                    
                    # –û–±—â–∏–π —Å–∫–æ—Ä–∏–Ω–≥ –∏–º–ø—É–ª—å—Å–∞
                    d8_momentum_score = (abs(d8_acceleration) * 0.4 + d8_volume_surge * 0.6)
                
                momentum_features.append({
                    'ts_group': ts_group,
                    'exchange': exchange,
                    'symbol': symbol,
                    'd8_momentum_score': d8_momentum_score,
                    'd8_acceleration': d8_acceleration,
                    'd8_volume_surge': d8_volume_surge
                })
        
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –æ—Å–Ω–æ–≤–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        momentum_df = pd.DataFrame(momentum_features)
        if not momentum_df.empty:
            features_df = features_df.merge(
                momentum_df[['ts_group', 'exchange', 'symbol', 'd8_momentum_score', 'd8_acceleration', 'd8_volume_surge']],
                on=['ts_group', 'exchange', 'symbol'],
                how='left'
            )
            
            # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
            features_df['d8_momentum_score'] = features_df['d8_momentum_score'].fillna(0.0)
            features_df['d8_acceleration'] = features_df['d8_acceleration'].fillna(0.0)
            features_df['d8_volume_surge'] = features_df['d8_volume_surge'].fillna(0.0)
            
            logger.info("–ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è D8 (–ò–º–ø—É–ª—å—Å) –¥–æ–±–∞–≤–ª–µ–Ω—ã")
        else:
            # –ï—Å–ª–∏ –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–µ –∏–∑–≤–ª–µ—á–µ–Ω—ã, –¥–æ–±–∞–≤–ª—è–µ–º –ø—É—Å—Ç—ã–µ –∫–æ–ª–æ–Ω–∫–∏
            features_df['d8_momentum_score'] = 0.0
            features_df['d8_acceleration'] = 0.0
            features_df['d8_volume_surge'] = 0.0
        
        return features_df

def run_features(
    book_df: pd.DataFrame,
    trades_df: pd.DataFrame,
    quotes_df: pd.DataFrame,
    config: Dict,
    output_dir: Path = Path("data/features")
) -> pd.DataFrame:
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤"""
    logger.info("–ó–∞–ø—É—Å–∫ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤...")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏–∑–≤–ª–µ–∫–∞—Ç–µ–ª—è
    extractor = DetectorFeatureExtractor(config)
    
    # –ü–æ–ª–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    features_df = extractor.run_feature_extraction(book_df, trades_df, quotes_df)
    
    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–∫–æ—Ä–∏–Ω–≥–æ–≤ –¥–ª—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤
    if not features_df.empty:
        features_df = calculate_detector_scores(features_df, config)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        output_path = output_dir
        output_path.mkdir(parents=True, exist_ok=True)
        
        # –û—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        features_path = output_path / "detector_features.parquet"
        features_df.to_parquet(features_path, engine="pyarrow", index=False)
        logger.info(f"–ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {len(features_df)} —Å—Ç—Ä–æ–∫ –≤ {features_path}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        save_detector_features_summary(features_df, output_path)
    
    logger.info("–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
    return features_df

def calculate_detector_scores(features_df: pd.DataFrame, config: Dict) -> pd.DataFrame:
    """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–∫–æ—Ä–∏–Ω–≥–æ–≤ –¥–ª—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤"""
    logger.info("–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–∫–æ—Ä–∏–Ω–≥–æ–≤ –¥–ª—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤...")
    
    if features_df.empty:
        return features_df
    
    df = features_df.copy()
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    numeric_columns = df.select_dtypes(include=[np.number]).columns
    for col in numeric_columns:
        if col not in ['ts_ns', 'ts_group']:
            df[f'{col}_normalized'] = (df[col] - df[col].mean()) / (df[col].std() + 1e-8)
    
    # –°–∫–æ—Ä–∏–Ω–≥ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞
    df['d1_score'] = df['d1_vacuum_score']
    df['d2_score'] = df['d2_side_ratio'] * df['d2_price_stable'].astype(int)
    df['d3_score'] = df['d3_volume_concentration']
    df['d4_score'] = df['d4_price_momentum'] * df['d4_volume_surge']
    df['d5_score'] = df['d5_reversal_probability']
    df['d6_score'] = (df['d6_bid_ask_imbalance'] > config.get('d6_imbalance', {}).get('bid_ask_ratio', 3.0)).astype(int) * df['d6_level_wall']
    df['d7_score'] = df['d7_spoofing_score'] * df['d7_quick_cancel'].astype(int)
    df['d8_score'] = df['d8_momentum_score']
    
    # –û–±—â–∏–π —Å–∫–æ—Ä–∏–Ω–≥ –≤—Å–µ—Ö –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤
    df['overall_detector_score'] = (
        df['d1_score'] + df['d2_score'] + df['d3_score'] + df['d4_score'] +
        df['d5_score'] + df['d6_score'] + df['d7_score'] + df['d8_score']
    ) / 8.0
    
    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞–º
    df['primary_detector'] = df[['d1_score', 'd2_score', 'd3_score', 'd4_score', 
                                 'd5_score', 'd6_score', 'd7_score', 'd8_score']].idxmax(axis=1)
    
    # –û—á–∏—Å—Ç–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏–π –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤
    df['primary_detector'] = df['primary_detector'].str.replace('_score', '')
    
    logger.info(f"–°–∫–æ—Ä–∏–Ω–≥–∏ –¥–ª—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤ –≤—ã—á–∏—Å–ª–µ–Ω—ã")
    return df

def save_detector_features_summary(features_df: pd.DataFrame, output_dir: Path):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–≤–æ–¥–∫–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤"""
    logger.info("–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–≤–æ–¥–∫–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤...")
    
    # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ –±–∏—Ä–∂–∞–º
    if not features_df.empty:
        exchange_summary = features_df.groupby('exchange').agg({
            'd1_score': 'mean',
            'd2_score': 'mean',
            'd3_score': 'mean',
            'd4_score': 'mean',
            'd5_score': 'mean',
            'd6_score': 'mean',
            'd7_score': 'mean',
            'd8_score': 'mean',
            'overall_detector_score': 'mean'
        }).round(6)
        
        exchange_summary.to_parquet(output_dir / "detector_exchange_summary.parquet", engine="pyarrow")
    
    # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ –æ—Å–Ω–æ–≤–Ω—ã–º –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞–º
    if 'primary_detector' in features_df.columns:
        detector_summary = features_df.groupby('primary_detector').agg({
            'overall_detector_score': ['mean', 'std', 'count'],
            'd1_score': 'mean',
            'd2_score': 'mean',
            'd3_score': 'mean',
            'd4_score': 'mean',
            'd5_score': 'mean',
            'd6_score': 'mean',
            'd7_score': 'mean',
            'd8_score': 'mean'
        }).round(6)
        
        detector_summary.to_parquet(output_dir / "detector_pattern_summary.parquet", engine="pyarrow")
    
    # –í—Ä–µ–º–µ–Ω–Ω–∞—è –∞–≥—Ä–µ–≥–∞—Ü–∏—è
    if not features_df.empty:
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫
        features_df['timestamp'] = pd.to_datetime(features_df['ts_ns'], unit='ns')
        features_df['hour'] = features_df['timestamp'].dt.hour
        features_df['date'] = features_df['timestamp'].dt.date
        
        # –ü–æ—á–∞—Å–æ–≤–∞—è –∞–≥—Ä–µ–≥–∞—Ü–∏—è
        hourly_summary = features_df.groupby(['date', 'hour']).agg({
            'd1_score': 'mean',
            'd2_score': 'mean',
            'd3_score': 'mean',
            'd4_score': 'mean',
            'd5_score': 'mean',
            'd6_score': 'mean',
            'd7_score': 'mean',
            'd8_score': 'mean',
            'overall_detector_score': 'mean'
        }).round(6)
        
        hourly_summary.to_parquet(output_dir / "detector_hourly_summary.parquet", engine="pyarrow")
    
    logger.info("–°–≤–æ–¥–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")


if __name__ == "__main__":
    """–ó–∞–ø—É—Å–∫ –±–ª–æ–∫–∞ 10 –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    import pandas as pd
    import logging
    import sys
    import os
    import time
    
    # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ base_block
    sys.path.append(os.path.dirname(__file__))
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')
    
    print("üß™ –¢–ï–°–¢ –ë–õ–û–ö–ê 10: Features")
    print("=" * 50)
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –±–ª–æ–∫–æ–≤
        book_df = pd.read_parquet("../../../data/test_small/book_top_small.parquet")
        trades_df = pd.read_parquet("../../../data/test_small/trades_small.parquet")
        quotes_df = pd.read_parquet("../../../data/quotes/quotes.parquet")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â—É—é –∫–æ–ª–æ–Ω–∫—É instrument –≤ quotes
        if 'instrument' not in quotes_df.columns:
            quotes_df['instrument'] = 'ETHUSDT'
        
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
        print(f"   - Book: {len(book_df)} —Å—Ç—Ä–æ–∫")
        print(f"   - Trades: {len(trades_df)} —Å—Ç—Ä–æ–∫")
        print(f"   - Quotes: {len(quotes_df)} —Å—Ç—Ä–æ–∫")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∫–∏–µ –±–∏—Ä–∂–∏ —É –Ω–∞—Å –µ—Å—Ç—å
        if not book_df.empty:
            exchanges = book_df['exchange'].unique()
            print(f"   - –ë–∏—Ä–∂–∏ –≤ book: {exchanges}")
        
        if not trades_df.empty:
            exchanges = trades_df['exchange'].unique()
            print(f"   - –ë–∏—Ä–∂–∏ –≤ trades: {exchanges}")
        
        if not quotes_df.empty:
            exchanges = quotes_df['exchange'].unique()
            print(f"   - –ë–∏—Ä–∂–∏ –≤ quotes: {exchanges}")
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        config = {
            'time': {
                'tick_size_ms': 10
            },
            'orderbook': {
                'top_levels': 10
            },
            'd1_vacuum': {
                'threshold': 0.1,
                'empty_levels': 3
            },
            'd2_absorption': {
                'min_side_ratio': 0.7,
                'max_price_move_ticks': 2
            },
            'd3_iceberg': {
                'same_price_trades': 10,
                'time_window_ms': 2000
            },
            'd4_stop_run': {
                'min_price_move_percent': 0.3,
                'time_window_ms': 2000
            },
            'd5_false_breakout': {
                'reversal_threshold_ratio': 0.7,
                'reversal_window_ms': 5000
            },
            'd6_imbalance': {
                'bid_ask_ratio': 3.0,
                'level_wall_multiplier': 5.0
            },
            'd7_spoofing': {
                'large_order_multiplier': 5.0,
                'cancel_time_window_ms': 1000
            },
            'd8_momentum': {
                'acceleration_window_ms': 3000,
                'min_volume_ratio': 3.0
            }
        }
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –±–ª–æ–∫ 10
        print("üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º –±–ª–æ–∫ 10: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤...")
        start_time = time.time()
        
        features_df = run_features(book_df, trades_df, quotes_df, config)
        
        execution_time = time.time() - start_time
        
        print(f"‚úÖ –ë–ª–æ–∫ 10 –≤—ã–ø–æ–ª–Ω–µ–Ω –∑–∞ {execution_time:.2f} —Å–µ–∫—É–Ω–¥!")
        print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        if not features_df.empty:
            print(f"   - –ü—Ä–∏–∑–Ω–∞–∫–∏: {len(features_df)} –∑–∞–ø–∏—Å–µ–π")
            print(f"   - –ë–∏—Ä–∂–∏ –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö: {features_df['exchange'].unique() if 'exchange' in features_df.columns else 'N/A'}")
            
            if 'primary_detector' in features_df.columns:
                detectors = features_df['primary_detector'].value_counts()
                print(f"   - –û—Å–Ω–æ–≤–Ω—ã–µ –¥–µ—Ç–µ–∫—Ç–æ—Ä—ã: {dict(detectors)}")
            
            if 'overall_detector_score' in features_df.columns:
                avg_score = features_df['overall_detector_score'].mean()
                print(f"   - –°—Ä–µ–¥–Ω–∏–π –æ–±—â–∏–π —Å–∫–æ—Ä–∏–Ω–≥: {avg_score:.4f}")
        
        print("‚úÖ –ë–ª–æ–∫ 10 —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω!")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
