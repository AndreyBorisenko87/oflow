"""
–ë–ª–æ–∫ 09: –ë–∞–∑–∏—Å —Å–ø–æ—Ç‚Äì—Ñ—å—é—á–µ—Ä—Å
–í—ã—á–∏—Å–ª–∏—Ç—å —Ä–∞–∑–Ω–∏—Ü—É/–ø—Ä–æ—Ü–µ–Ω—Ç/—Å–∫–æ–ª—å–∑—è—â–∏–µ –∏ –ø–æ–º–µ—Ç–∫–∏ –∞–Ω–æ–º–∞–ª–∏–π
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional
from pathlib import Path
import logging
import time

logger = logging.getLogger(__name__)

class BasisAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –±–∞–∑–∏—Å–∞ —Å–ø–æ—Ç-—Ñ—å—é—á–µ—Ä—Å"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.sync_window_ms = config.get('sync', {}).get('max_lag_ms', 5000)
        self.tick_size_ms = config.get('time', {}).get('tick_size_ms', 10)
        self.anomaly_threshold = config.get('anomaly', {}).get('threshold', 2.0)
        self.ma_windows = config.get('moving_averages', [5, 10, 20, 50])
        self.volatility_window = config.get('volatility', {}).get('window', 20)
        
    def run_basis_analysis(
        self,
        spot_df: pd.DataFrame,
        futures_df: pd.DataFrame
    ) -> pd.DataFrame:
        """–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –±–∞–∑–∏—Å–∞ —Å–ø–æ—Ç-—Ñ—å—é—á–µ—Ä—Å"""
        start_time = time.time()
        logger.info("=== –ù–∞—á–∞–ª–æ –∞–Ω–∞–ª–∏–∑–∞ –±–∞–∑–∏—Å–∞ —Å–ø–æ—Ç-—Ñ—å—é—á–µ—Ä—Å ===")
        
        # –≠—Ç–∞–ø 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        logger.info("–≠—Ç–∞–ø 1/4: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...")
        synced_spot, synced_futures = self._synchronize_data(spot_df, futures_df)
        
        # –≠—Ç–∞–ø 2: –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –±–∞–∑–∏—Å–∞
        logger.info("–≠—Ç–∞–ø 2/4: –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –±–∞–∑–∏—Å–∞...")
        basis_df = self._calculate_basis(synced_spot, synced_futures)
        
        # –≠—Ç–∞–ø 3: –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π
        logger.info("–≠—Ç–∞–ø 3/4: –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π...")
        basis_df = self._detect_anomalies(basis_df)
        
        # –≠—Ç–∞–ø 4: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        logger.info("–≠—Ç–∞–ø 4/4: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏...")
        basis_df = self._calculate_additional_metrics(basis_df)
        
        # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ
        duration = time.time() - start_time
        logger.info(f"–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {duration:.2f}—Å")
        logger.info(f"–ë–∞–∑–∏—Å –≤—ã—á–∏—Å–ª–µ–Ω: {len(basis_df)} –∑–∞–ø–∏—Å–µ–π")
        logger.info("=== –ê–Ω–∞–ª–∏–∑ –±–∞–∑–∏—Å–∞ —Å–ø–æ—Ç-—Ñ—å—é—á–µ—Ä—Å –∑–∞–≤–µ—Ä—à–µ–Ω ===")
        
        return basis_df
    
    def _synchronize_data(
        self,
        spot_df: pd.DataFrame,
        futures_df: pd.DataFrame
    ) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö —Å–ø–æ—Ç –∏ —Ñ—å—é—á–µ—Ä—Å –ø–æ –≤—Ä–µ–º–µ–Ω–∏"""
        logger.info("–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...")
        
        if spot_df.empty or futures_df.empty:
            logger.warning("–û–¥–∏–Ω –∏–∑ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–æ–≤ –ø—É—Å—Ç")
            return spot_df, futures_df
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        required_columns = ['ts_ns', 'exchange', 'symbol', 'price']
        missing_spot = [col for col in required_columns if col not in spot_df.columns]
        missing_futures = [col for col in required_columns if col not in futures_df.columns]
        
        if missing_spot or missing_futures:
            logger.error(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–ª–æ–Ω–∫–∏: spot={missing_spot}, futures={missing_futures}")
            return pd.DataFrame(), pd.DataFrame()
        
        # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        synced_spot = self._synchronize_quotes(spot_df, 'spot')
        synced_futures = self._synchronize_quotes(futures_df, 'futures')
        
        logger.info(f"–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: spot={len(synced_spot)}, futures={len(synced_futures)}")
        return synced_spot, synced_futures
    
    def _synchronize_quotes(self, quotes_df: pd.DataFrame, market_type: str) -> pd.DataFrame:
        """–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –∫–æ—Ç–∏—Ä–æ–≤–æ–∫ –ø–æ –≤—Ä–µ–º–µ–Ω–∏"""
        df = quotes_df.copy()
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        df = df.sort_values('ts_ns').reset_index(drop=True)
        
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ —Å —É—á–µ—Ç–æ–º —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏
        df['ts_group'] = (df['ts_ns'] // (self.tick_size_ms * 1_000_000)).astype(int)
        
        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ –≥—Ä—É–ø–ø–∞–º –≤—Ä–µ–º–µ–Ω–∏ –∏ –±–∏—Ä–∂–∞–º
        synced_quotes = df.groupby(['ts_group', 'exchange', 'symbol']).agg({
            'ts_ns': 'first',
            'price': 'last'  # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Ü–µ–Ω—É –≤ –≥—Ä—É–ø–ø–µ
        }).reset_index()
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏
        synced_quotes = synced_quotes.drop('ts_group', axis=1)
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Ä—ã–Ω–∫–∞
        synced_quotes['market_type'] = market_type
        
        return synced_quotes
    
    def _calculate_basis(
        self,
        spot_df: pd.DataFrame,
        futures_df: pd.DataFrame
    ) -> pd.DataFrame:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –±–∞–∑–∏—Å–∞ —Å–ø–æ—Ç-—Ñ—å—é—á–µ—Ä—Å"""
        logger.info("–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –±–∞–∑–∏—Å–∞...")
        
        if spot_df.empty or futures_df.empty:
            return pd.DataFrame()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –≥—Ä—É–ø–ø –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏
        spot_df = spot_df.copy()
        futures_df = futures_df.copy()
        
        spot_df['ts_group'] = (spot_df['ts_ns'] // (self.tick_size_ms * 1_000_000)).astype(int)
        futures_df['ts_group'] = (futures_df['ts_ns'] // (self.tick_size_ms * 1_000_000)).astype(int)
        
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–º –≥—Ä—É–ø–ø–∞–º
        basis_records = []
        total_groups = min(spot_df['ts_group'].nunique(), futures_df['ts_group'].nunique())
        
        for i, (ts_group, spot_group) in enumerate(spot_df.groupby('ts_group')):
            if i % 1000 == 0:  # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–µ 1000 –≥—Ä—É–ø–ø
                logger.info(f"–ü—Ä–æ–≥—Ä–µ—Å—Å –±–∞–∑–∏—Å–∞: {i+1}/{total_groups} –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –≥—Ä—É–ø–ø")
            
            # –ü–æ–∏—Å–∫ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö —Ñ—å—é—á–µ—Ä—Å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            futures_group = futures_df[futures_df['ts_group'] == ts_group]
            
            if futures_group.empty:
                continue
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –±–∞–∑–∏—Å–∞ –¥–ª—è –∫–∞–∂–¥–æ–π –ø–∞—Ä—ã —Å–ø–æ—Ç-—Ñ—å—é—á–µ—Ä—Å
            for _, spot_row in spot_group.iterrows():
                for _, futures_row in futures_group.iterrows():
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å–∏–º–≤–æ–ª–æ–≤ (–±–∞–∑–æ–≤—ã–π –∞–∫—Ç–∏–≤)
                    if not self._symbols_match(spot_row['symbol'], futures_row['symbol']):
                        continue
                    
                    spot_price = spot_row['price']
                    futures_price = futures_row['price']
                    
                    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –±–∞–∑–∏—Å–∞
                    basis_abs = futures_price - spot_price
                    basis_rel = (basis_abs / spot_price) * 100 if spot_price > 0 else 0
                    
                    # –í–∞–ª–∏–¥–∞—Ü–∏—è –±–∞–∑–∏—Å–∞
                    if not self._validate_basis(basis_abs, basis_rel):
                        continue
                    
                    basis_records.append({
                        'ts_ns': spot_row['ts_ns'],
                        'ts_group': ts_group,
                        'spot_exchange': spot_row['exchange'],
                        'futures_exchange': futures_row['exchange'],
                        'symbol': spot_row['symbol'],
                        'spot_price': spot_price,
                        'futures_price': futures_price,
                        'basis_abs': basis_abs,
                        'basis_rel': basis_rel,
                        'spot_market': 'spot',
                        'futures_market': 'futures'
                    })
        
        basis_df = pd.DataFrame(basis_records)
        
        if not basis_df.empty:
            # –£–¥–∞–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏
            basis_df = basis_df.drop('ts_group', axis=1)
            
            # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
            basis_df = basis_df.sort_values('ts_ns').reset_index(drop=True)
            
            logger.info(f"–ë–∞–∑–∏—Å –≤—ã—á–∏—Å–ª–µ–Ω: {len(basis_df)} –∑–∞–ø–∏—Å–µ–π")
        else:
            logger.warning("–ë–∞–∑–∏—Å –Ω–µ –≤—ã—á–∏—Å–ª–µ–Ω - –Ω–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
        
        return basis_df
    
    def _symbols_match(self, spot_symbol: str, futures_symbol: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Å–∏–º–≤–æ–ª–æ–≤ —Å–ø–æ—Ç –∏ —Ñ—å—é—á–µ—Ä—Å"""
        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞: —É–±–∏—Ä–∞–µ–º —Å—É—Ñ—Ñ–∏–∫—Å—ã —Ñ—å—é—á–µ—Ä—Å–æ–≤
        spot_base = spot_symbol.replace('-USDT', '').replace('USDT', '')
        futures_base = futures_symbol.replace('-PERP', '').replace('-SWAP', '').replace('-USDT', '').replace('USDT', '')
        
        return spot_base == futures_base
    
    def _validate_basis(self, basis_abs: float, basis_rel: float) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–Ω–æ–≥–æ –±–∞–∑–∏—Å–∞"""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ä–∞–∑—É–º–Ω—ã–µ –ø—Ä–µ–¥–µ–ª—ã
        max_basis_rel = self.config.get('basis', {}).get('max_basis_rel', 50.0)  # 50%
        min_basis_rel = self.config.get('basis', {}).get('min_basis_rel', -50.0)  # -50%
        
        return min_basis_rel <= basis_rel <= max_basis_rel
    
    def _detect_anomalies(self, basis_df: pd.DataFrame) -> pd.DataFrame:
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π –≤ –±–∞–∑–∏—Å–µ"""
        logger.info("–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π...")
        
        if basis_df.empty:
            return basis_df
        
        df = basis_df.copy()
        
        # –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ –¥–ª—è –±–∞–∑–∏—Å–∞
        for window in self.ma_windows:
            df[f'basis_rel_ma_{window}'] = df['basis_rel'].rolling(window=window).mean()
            df[f'basis_abs_ma_{window}'] = df['basis_abs'].rolling(window=window).mean()
        
        # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –±–∞–∑–∏—Å–∞
        df['basis_volatility'] = df['basis_rel'].rolling(window=self.volatility_window).std()
        
        # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π
        df = self._calculate_anomaly_scores(df)
        
        # –ü–æ–º–µ—Ç–∫–∏ –∞–Ω–æ–º–∞–ª–∏–π
        df = self._flag_anomalies(df)
        
        logger.info(f"–ê–Ω–æ–º–∞–ª–∏–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã: {df['anomaly_flag'].sum()} –∑–∞–ø–∏—Å–µ–π")
        return df
    
    def _calculate_anomaly_scores(self, df: pd.DataFrame) -> pd.DataFrame:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–∫–æ—Ä–∏–Ω–≥–∞ –∞–Ω–æ–º–∞–ª–∏–π"""
        # –ë–∞–∑–æ–≤—ã–π —Å–∫–æ—Ä–∏–Ω–≥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –æ—Ç —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ —Å—Ä–µ–¥–Ω–µ–≥–æ
        df['anomaly_score'] = 0.0
        
        for window in self.ma_windows:
            ma_col = f'basis_rel_ma_{window}'
            if ma_col in df.columns:
                # –û—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –æ—Ç MA
                deviation = abs(df['basis_rel'] - df[ma_col])
                ma_score = deviation / (df[ma_col].abs() + 1e-8)  # –ò–∑–±–µ–≥–∞–µ–º –¥–µ–ª–µ–Ω–∏—è –Ω–∞ 0
                
                # –ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ —Å–∫–æ—Ä–∏–Ω–≥–∞
                df['anomaly_score'] += ma_score
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–∫–æ—Ä–∏–Ω–≥–∞
        if df['anomaly_score'].max() > 0:
            df['anomaly_score'] = df['anomaly_score'] / df['anomaly_score'].max()
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã
        if 'basis_volatility' in df.columns:
            # –°–∫–æ—Ä–∏–Ω–≥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
            volatility_score = df['basis_volatility'] / (df['basis_volatility'].mean() + 1e-8)
            df['anomaly_score'] += volatility_score * 0.3  # –í–µ—Å 30%
        
        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Å–∫–æ—Ä–∏–Ω–≥–∞
        df['anomaly_score'] = df['anomaly_score'].clip(0, 1)
        
        return df
    
    def _flag_anomalies(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ü–æ–º–µ—Ç–∫–∞ –∞–Ω–æ–º–∞–ª–∏–π"""
        # –ü–æ—Ä–æ–≥ –¥–ª—è –ø–æ–º–µ—Ç–∫–∏ –∞–Ω–æ–º–∞–ª–∏–π
        threshold = self.anomaly_threshold
        
        # –ê–Ω–æ–º–∞–ª–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–∫–æ—Ä–∏–Ω–≥–∞
        df['anomaly_flag'] = df['anomaly_score'] > threshold
        
        # –ê–Ω–æ–º–∞–ª–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
        if 'basis_volatility' in df.columns:
            volatility_threshold = df['basis_volatility'].quantile(0.95)  # 95-–π –ø—Ä–æ—Ü–µ–Ω—Ç–∏–ª—å
            volatility_anomalies = df['basis_volatility'] > volatility_threshold
            df['anomaly_flag'] = df['anomaly_flag'] | volatility_anomalies
        
        # –ê–Ω–æ–º–∞–ª–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑–∫–∏—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π
        if 'basis_rel' in df.columns:
            basis_change = df['basis_rel'].diff().abs()
            change_threshold = basis_change.quantile(0.95)  # 95-–π –ø—Ä–æ—Ü–µ–Ω—Ç–∏–ª—å
            change_anomalies = basis_change > change_threshold
            df['anomaly_flag'] = df['anomaly_flag'] | change_anomalies
        
        return df
    
    def _calculate_additional_metrics(self, basis_df: pd.DataFrame) -> pd.DataFrame:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫"""
        logger.info("–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫...")
        
        if basis_df.empty:
            return basis_df
        
        df = basis_df.copy()
        
        # –ò–∑–º–µ–Ω–µ–Ω–∏—è –±–∞–∑–∏—Å–∞
        df['basis_rel_change'] = df['basis_rel'].diff()
        df['basis_rel_change_pct'] = (df['basis_rel_change'] / df['basis_rel'].shift(1)) * 100
        
        # –¢—Ä–µ–Ω–¥—ã –±–∞–∑–∏—Å–∞
        df['basis_trend'] = df['basis_rel'].rolling(window=10).apply(
            lambda x: 1 if x.iloc[-1] > x.iloc[0] else (-1 if x.iloc[-1] < x.iloc[0] else 0)
        )
        
        # –ö–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü–∏—è/–¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏—è
        df['convergence'] = df.apply(
            lambda row: 1.0 if abs(row['basis_rel']) < abs(row.get('basis_rel_ma_20', 0)) else 0.0, axis=1
        )
        
        # Z-score –±–∞–∑–∏—Å–∞
        if 'basis_rel_ma_20' in df.columns and 'basis_volatility' in df.columns:
            df['basis_zscore'] = (df['basis_rel'] - df['basis_rel_ma_20']) / (df['basis_volatility'] + 1e-8)
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –±–∞–∑–∏—Å–∞
        df['basis_category'] = df['basis_rel'].apply(self._categorize_basis)
        
        return df
    
    def _categorize_basis(self, basis_rel: float) -> str:
        """–ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –±–∞–∑–∏—Å–∞"""
        if pd.isna(basis_rel):
            return 'unknown'
        
        if basis_rel > 5.0:
            return 'strong_contango'
        elif basis_rel > 1.0:
            return 'contango'
        elif basis_rel > -1.0:
            return 'near_par'
        elif basis_rel > -5.0:
            return 'backwardation'
        else:
            return 'strong_backwardation'

def run_basis(
    spot_df: pd.DataFrame,
    futures_df: pd.DataFrame,
    config: Dict,
    output_dir: Path = Path("data/basis")
) -> pd.DataFrame:
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ –±–∞–∑–∏—Å–∞"""
    logger.info("–ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ –±–∞–∑–∏—Å–∞ —Å–ø–æ—Ç-—Ñ—å—é—á–µ—Ä—Å...")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
    analyzer = BasisAnalyzer(config)
    
    # –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑
    basis_df = analyzer.run_basis_analysis(spot_df, futures_df)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if not basis_df.empty:
        output_path = output_dir
        output_path.mkdir(parents=True, exist_ok=True)
        
        # –û—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª –±–∞–∑–∏—Å–∞
        basis_path = output_path / "basis_analyzed.parquet"
        basis_df.to_parquet(basis_path, engine="pyarrow", index=False)
        logger.info(f"–ë–∞–∑–∏—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {len(basis_df)} —Å—Ç—Ä–æ–∫ –≤ {basis_path}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        save_aggregated_data(basis_df, output_path)
    
    logger.info("–ê–Ω–∞–ª–∏–∑ –±–∞–∑–∏—Å–∞ –∑–∞–≤–µ—Ä—à–µ–Ω")
    return basis_df

def save_aggregated_data(basis_df: pd.DataFrame, output_dir: Path):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    logger.info("–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    
    # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ –±–∏—Ä–∂–∞–º
    if not basis_df.empty:
        exchange_summary = basis_df.groupby(['spot_exchange', 'futures_exchange']).agg({
            'basis_rel': ['mean', 'std', 'min', 'max'],
            'basis_abs': ['mean', 'std', 'min', 'max'],
            'anomaly_score': 'mean',
            'anomaly_flag': 'sum'
        }).round(6)
        
        exchange_summary.to_parquet(output_dir / "exchange_basis_summary.parquet", engine="pyarrow")
    
    # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –±–∞–∑–∏—Å–∞
    if 'basis_category' in basis_df.columns:
        category_summary = basis_df.groupby('basis_category').agg({
            'basis_rel': ['mean', 'std', 'count'],
            'anomaly_score': 'mean',
            'anomaly_flag': 'sum'
        }).round(6)
        
        category_summary.to_parquet(output_dir / "category_basis_summary.parquet", engine="pyarrow")
    
    # –í—Ä–µ–º–µ–Ω–Ω–∞—è –∞–≥—Ä–µ–≥–∞—Ü–∏—è
    if not basis_df.empty:
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫
        basis_df['timestamp'] = pd.to_datetime(basis_df['ts_ns'], unit='ns')
        basis_df['hour'] = basis_df['timestamp'].dt.hour
        basis_df['date'] = basis_df['timestamp'].dt.date
        
        # –ü–æ—á–∞—Å–æ–≤–∞—è –∞–≥—Ä–µ–≥–∞—Ü–∏—è
        hourly_summary = basis_df.groupby(['date', 'hour']).agg({
            'basis_rel': ['mean', 'std', 'min', 'max'],
            'basis_abs': ['mean', 'std'],
            'anomaly_score': 'mean',
            'anomaly_flag': 'sum'
        }).round(6)
        
        hourly_summary.to_parquet(output_dir / "hourly_basis_summary.parquet", engine="pyarrow")
    
    logger.info("–ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")


if __name__ == "__main__":
    """–ó–∞–ø—É—Å–∫ –±–ª–æ–∫–∞ 09 –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    import pandas as pd
    import logging
    import sys
    import os
    import time
    
    # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ base_block
    sys.path.append(os.path.dirname(__file__))
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')
    
    print("üß™ –¢–ï–°–¢ –ë–õ–û–ö–ê 09: Basis")
    print("=" * 50)
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –±–ª–æ–∫–æ–≤
        quotes_df = pd.read_parquet("../../../data/quotes/quotes.parquet")
        trades_df = pd.read_parquet("../../../data/test_small/trades_small.parquet")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â—É—é –∫–æ–ª–æ–Ω–∫—É instrument –≤ quotes
        if 'instrument' not in quotes_df.columns:
            quotes_df['instrument'] = 'ETHUSDT'
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ spot –∏ futures –¥–∞–Ω–Ω—ã–µ
        spot_df = quotes_df[quotes_df['exchange'].isin(['binance', 'bybit', 'okx'])].copy()
        futures_df = quotes_df[quotes_df['exchange'].isin(['binance', 'bybit', 'okx'])].copy()
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫–∏ –¥–ª—è symbol (–∏—Å–ø–æ–ª—å–∑—É–µ–º instrument)
        spot_df['symbol'] = spot_df['instrument']
        futures_df['symbol'] = futures_df['instrument']
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫–∏ –¥–ª—è price (–∏—Å–ø–æ–ª—å–∑—É–µ–º mid)
        if 'price' not in spot_df.columns and 'mid' in spot_df.columns:
            spot_df['price'] = spot_df['mid']
        if 'price' not in futures_df.columns and 'mid' in futures_df.columns:
            futures_df['price'] = futures_df['mid']
        
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –±–∞–∑–∏—Å–∞:")
        print(f"   - Spot: {len(spot_df)} —Å—Ç—Ä–æ–∫")
        print(f"   - Futures: {len(futures_df)} —Å—Ç—Ä–æ–∫")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∫–∏–µ –±–∏—Ä–∂–∏ —É –Ω–∞—Å –µ—Å—Ç—å
        if not spot_df.empty:
            exchanges = spot_df['exchange'].unique()
            print(f"   - –ë–∏—Ä–∂–∏ –≤ spot: {exchanges}")
        
        if not futures_df.empty:
            exchanges = futures_df['exchange'].unique()
            print(f"   - –ë–∏—Ä–∂–∏ –≤ futures: {exchanges}")
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –±–∞–∑–∏—Å–∞
        config = {
            'sync': {
                'max_lag_ms': 5000      # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –±–∏—Ä–∂–∞–º–∏
            },
            'time': {
                'tick_size_ms': 10      # –†–∞–∑–º–µ—Ä –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ç–∏–∫–∞
            },
            'anomaly': {
                'threshold': 2.0        # –ü–æ—Ä–æ–≥ –∞–Ω–æ–º–∞–ª–∏–∏
            },
            'moving_averages': [5, 10, 20, 50],
            'volatility': {
                'window': 20
            }
        }
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –±–ª–æ–∫ 09
        print("üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º –±–ª–æ–∫ 09: –ê–Ω–∞–ª–∏–∑ –±–∞–∑–∏—Å–∞ —Å–ø–æ—Ç-—Ñ—å—é—á–µ—Ä—Å...")
        start_time = time.time()
        
        basis_df = run_basis(spot_df, futures_df, config)
        
        execution_time = time.time() - start_time
        
        print(f"‚úÖ –ë–ª–æ–∫ 09 –≤—ã–ø–æ–ª–Ω–µ–Ω –∑–∞ {execution_time:.2f} —Å–µ–∫—É–Ω–¥!")
        print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –±–∞–∑–∏—Å–∞:")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        if not basis_df.empty:
            print(f"   - –ë–∞–∑–∏—Å: {len(basis_df)} –∑–∞–ø–∏—Å–µ–π")
            print(f"   - –ë–∏—Ä–∂–∏ –≤ –±–∞–∑–∏—Å–µ: {basis_df['spot_exchange'].unique() if 'spot_exchange' in basis_df.columns else 'N/A'}")
            
            if 'basis_category' in basis_df.columns:
                categories = basis_df['basis_category'].value_counts()
                print(f"   - –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –±–∞–∑–∏—Å–∞: {dict(categories)}")
        
        print("‚úÖ –ë–ª–æ–∫ 09 —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω!")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
