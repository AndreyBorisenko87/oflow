"""
–ë–ª–æ–∫ 13: –°–∫–∞–Ω–µ—Ä
–û–±—Ö–æ–¥ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤/–¥–∞—Ç/–±–∏—Ä–∂ –∏ –∑–∞–ø—É—Å–∫ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional, Set
from pathlib import Path
import logging
import time
import json
from datetime import datetime, timedelta
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
from .detectors.detector_runner import run_detectors, create_detectors_from_config

logger = logging.getLogger(__name__)

class DataScanner:
    """–°–∫–∞–Ω–µ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–ø—É—Å–∫–∞ –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.supported_data_types = config.get('data_types', ['quotes', 'book_top', 'tape', 'nbbo', 'basis', 'features'])
        self.batch_size = config.get('processing', {}).get('batch_size', 10)
        self.max_workers = config.get('processing', {}).get('max_workers', 4)
        self.memory_limit_gb = config.get('processing', {}).get('memory_limit_gb', 8)
        self.date_range = config.get('filters', {}).get('date_range', {})
        self.exchanges = config.get('filters', {}).get('exchanges', [])
        self.symbols = config.get('filters', {}).get('symbols', [])
        
    def scan_canonical_files(self, data_dir: Path) -> Dict[str, List[Path]]:
        """–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏—Ö —Ñ–∞–π–ª–æ–≤ –¥–∞–Ω–Ω—ã—Ö"""
        start_time = time.time()
        logger.info("=== –ù–∞—á–∞–ª–æ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–æ–≤ ===")
        logger.info(f"–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {data_dir}")
        
        if not data_dir.exists():
            logger.error(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {data_dir}")
            return {}
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–Ω–¥–µ–∫—Å —Ñ–∞–π–ª–æ–≤, –µ—Å–ª–∏ –æ–Ω —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        index_path = data_dir / "file_index.json"
        file_index = None
        if index_path.exists():
            try:
                with open(index_path, 'r', encoding='utf-8') as f:
                    file_index = json.load(f)
                logger.info("‚úì –ó–∞–≥—Ä—É–∂–µ–Ω –∏–Ω–¥–µ–∫—Å —Ñ–∞–π–ª–æ–≤")
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–Ω–¥–µ–∫—Å —Ñ–∞–π–ª–æ–≤: {e}")
        
        # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        found_files = {data_type: [] for data_type in self.supported_data_types}
        
        # –ü—Ä–æ–≥—Ä–µ—Å—Å —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
        if file_index and 'files' in file_index:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–µ–∫—Å —Ñ–∞–π–ª–æ–≤
            logger.info("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞...")
            
            for data_type in self.supported_data_types:
                if data_type in file_index['files']:
                    for file_info in file_index['files'][data_type]:
                        file_path = data_dir / file_info['path']
                        if file_path.exists() and self._passes_filters(file_info, file_path):
                            found_files[data_type].append(file_path)
        else:
            # –†—É—á–Ω–æ–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
            logger.info("–†—É—á–Ω–æ–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤...")
            
            for data_type in self.supported_data_types:
                type_dir = data_dir / data_type
                if type_dir.exists():
                    parquet_files = list(type_dir.glob("*.parquet"))
                    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(parquet_files)} —Ñ–∞–π–ª–æ–≤ {data_type}")
                    
                    for file_path in parquet_files:
                        if self._passes_filters({}, file_path):
                            found_files[data_type].append(file_path)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_files = sum(len(files) for files in found_files.values())
        duration = time.time() - start_time
        
        logger.info(f"=== –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {duration:.2f}—Å ===")
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –ø–æ —Ç–∏–ø–∞–º:")
        for data_type, files in found_files.items():
            if files:
                logger.info(f"  - {data_type}: {len(files)} —Ñ–∞–π–ª–æ–≤")
        logger.info(f"–í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {total_files}")
        
        return found_files
    
    def _passes_filters(self, file_info: Dict, file_path: Path) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–∞ –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ñ–∏–ª—å—Ç—Ä–∞–º"""
        # –§–∏–ª—å—Ç—Ä –ø–æ –¥–∞—Ç–µ
        if self.date_range:
            if not self._check_date_filter(file_info, file_path):
                return False
        
        # –§–∏–ª—å—Ç—Ä –ø–æ –±–∏—Ä–∂–∞–º
        if self.exchanges:
            if not self._check_exchange_filter(file_path):
                return False
        
        # –§–∏–ª—å—Ç—Ä –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        if self.symbols:
            if not self._check_symbol_filter(file_path):
                return False
        
        return True
    
    def _check_date_filter(self, file_info: Dict, file_path: Path) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∏–ª—å—Ç—Ä–∞ –ø–æ –¥–∞—Ç–µ"""
        start_date = self.date_range.get('start')
        end_date = self.date_range.get('end')
        
        if not start_date and not end_date:
            return True
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –∏–ª–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        file_date = None
        
        if 'date_range' in file_info:
            file_date = file_info['date_range']['start']
        else:
            # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ (—Ñ–æ—Ä–º–∞—Ç: type_YYYYMMDD_YYYYMMDD.parquet)
            filename = file_path.name
            date_match = re.search(r'_(\d{8})_\d{8}\.parquet$', filename)
            if date_match:
                file_date = date_match.group(1)
        
        if file_date:
            if start_date and file_date < start_date:
                return False
            if end_date and file_date > end_date:
                return False
        
        return True
    
    def _check_exchange_filter(self, file_path: Path) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∏–ª—å—Ç—Ä–∞ –ø–æ –±–∏—Ä–∂–∞–º"""
        # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –µ—Å–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –±–∏—Ä–∂–∏ –µ—Å—Ç—å –≤ –ø—É—Ç–∏ —Ñ–∞–π–ª–∞
        file_str = str(file_path).lower()
        return any(exchange.lower() in file_str for exchange in self.exchanges)
    
    def _check_symbol_filter(self, file_path: Path) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∏–ª—å—Ç—Ä–∞ –ø–æ —Å–∏–º–≤–æ–ª–∞–º"""
        # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –µ—Å–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ —Å–∏–º–≤–æ–ª–∞ –µ—Å—Ç—å –≤ –ø—É—Ç–∏ —Ñ–∞–π–ª–∞
        file_str = str(file_path).lower()
        return any(symbol.lower() in file_str for symbol in self.symbols)
    
    def load_batch_data(self, file_batch: Dict[str, List[Path]]) -> Dict[str, pd.DataFrame]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø–∞–∫–µ—Ç–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ñ–∞–π–ª–æ–≤"""
        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –ø–∞–∫–µ—Ç–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        batch_data = {}
        
        for data_type, files in file_batch.items():
            if not files:
                continue
                
            logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ {data_type}: {len(files)} —Ñ–∞–π–ª–æ–≤")
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ —Ç–∏–ø–∞ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ
            dfs = []
            for i, file_path in enumerate(files):
                if i % 5 == 0:  # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–µ 5 —Ñ–∞–π–ª–æ–≤
                    logger.info(f"  –ü—Ä–æ–≥—Ä–µ—Å—Å {data_type}: {i+1}/{len(files)} —Ñ–∞–π–ª–æ–≤")
                
                try:
                    df = pd.read_parquet(file_path)
                    if not df.empty:
                        dfs.append(df)
                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {file_path}: {e}")
            
            # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
            if dfs:
                combined_df = pd.concat(dfs, ignore_index=True)
                combined_df = combined_df.sort_values('ts_ns').reset_index(drop=True)
                batch_data[data_type] = combined_df
                logger.info(f"‚úì {data_type}: {len(combined_df)} –∑–∞–ø–∏—Å–µ–π –∑–∞–≥—Ä—É–∂–µ–Ω–æ")
            else:
                logger.warning(f"‚óã {data_type}: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏")
        
        return batch_data
    
    def create_batches(self, found_files: Dict[str, List[Path]]) -> List[Dict[str, List[Path]]]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–∫–µ—Ç–æ–≤ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        logger.info(f"–°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–∫–µ—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (—Ä–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞: {self.batch_size})...")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤
        all_files = []
        for data_type, files in found_files.items():
            for file_path in files:
                all_files.append((data_type, file_path))
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–∫–µ—Ç–æ–≤
        batches = []
        for i in range(0, len(all_files), self.batch_size):
            batch_files = all_files[i:i + self.batch_size]
            
            # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Ç–∏–ø–∞–º –¥–∞–Ω–Ω—ã—Ö
            batch = {data_type: [] for data_type in self.supported_data_types}
            for data_type, file_path in batch_files:
                batch[data_type].append(file_path)
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ–ø—É—Å—Ç—ã–µ –ø–∞–∫–µ—Ç—ã
            if any(files for files in batch.values()):
                batches.append(batch)
        
        logger.info(f"–°–æ–∑–¥–∞–Ω–æ {len(batches)} –ø–∞–∫–µ—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        return batches
    
    def process_batch(self, batch: Dict[str, List[Path]], detectors: List, output_dir: Path, batch_num: int) -> Dict[str, pd.DataFrame]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –ø–∞–∫–µ—Ç–∞ –¥–∞–Ω–Ω—ã—Ö"""
        logger.info(f"=== –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞–∫–µ—Ç–∞ {batch_num} ===")
        
        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–∞–∫–µ—Ç–∞
            batch_data = self.load_batch_data(batch)
            
            if not batch_data:
                logger.warning(f"–ü–∞–∫–µ—Ç {batch_num}: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
                return {}
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤
            required_types = ['book_top', 'quotes', 'tape']
            missing_types = [t for t in required_types if t not in batch_data or batch_data[t].empty]
            
            if missing_types:
                logger.warning(f"–ü–∞–∫–µ—Ç {batch_num}: –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –¥–∞–Ω–Ω—ã–µ {missing_types}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –¥–µ—Ç–µ–∫—Ü–∏—é")
                return {}
            
            # –ó–∞–ø—É—Å–∫ –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤
            logger.info(f"–ü–∞–∫–µ—Ç {batch_num}: –∑–∞–ø—É—Å–∫ {len(detectors)} –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤...")
            batch_output_dir = output_dir / f"batch_{batch_num:03d}"
            results = run_detectors(detectors, batch_data, batch_output_dir, use_progress=True)
            
            logger.info(f"‚úì –ü–∞–∫–µ—Ç {batch_num}: –æ–±—Ä–∞–±–æ—Ç–∞–Ω")
            return results
            
        except Exception as e:
            logger.error(f"‚úó –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–∞–∫–µ—Ç–∞ {batch_num}: {e}")
            import traceback
            traceback.print_exc()
            return {}
    
    def run_full_scan(self, data_dir: Path, output_dir: Path, detectors_config: Dict) -> Dict[str, pd.DataFrame]:
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –¥–µ—Ç–µ–∫—Ü–∏–∏"""
        start_time = time.time()
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –¥–µ—Ç–µ–∫—Ü–∏–∏")
        logger.info("=" * 80)
        
        # –≠—Ç–∞–ø 1: –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤
        logger.info("–≠—Ç–∞–ø 1/4: –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏—Ö —Ñ–∞–π–ª–æ–≤...")
        found_files = self.scan_canonical_files(data_dir)
        
        if not any(files for files in found_files.values()):
            logger.warning("–ù–µ—Ç —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            return {}
        
        # –≠—Ç–∞–ø 2: –°–æ–∑–¥–∞–Ω–∏–µ –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤
        logger.info("–≠—Ç–∞–ø 2/4: –°–æ–∑–¥–∞–Ω–∏–µ –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤...")
        detectors = create_detectors_from_config(detectors_config)
        
        if not detectors:
            logger.warning("–ù–µ—Ç –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è –∑–∞–ø—É—Å–∫–∞")
            return {}
        
        # –≠—Ç–∞–ø 3: –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–∫–µ—Ç–æ–≤
        logger.info("–≠—Ç–∞–ø 3/4: –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–∫–µ—Ç–æ–≤ –¥–∞–Ω–Ω—ã—Ö...")
        batches = self.create_batches(found_files)
        
        # –≠—Ç–∞–ø 4: –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞–∫–µ—Ç–æ–≤
        logger.info("–≠—Ç–∞–ø 4/4: –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞–∫–µ—Ç–æ–≤...")
        all_results = {}
        
        if self.max_workers > 1:
            # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
            logger.info(f"–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ {len(batches)} –ø–∞–∫–µ—Ç–æ–≤ ({self.max_workers} –≤–æ—Ä–∫–µ—Ä–æ–≤)...")
            
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = {
                    executor.submit(self.process_batch, batch, detectors, output_dir, i+1): i+1 
                    for i, batch in enumerate(batches)
                }
                
                for future in as_completed(futures):
                    batch_num = futures[future]
                    try:
                        batch_results = future.result()
                        
                        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                        for detector_name, events in batch_results.items():
                            if detector_name not in all_results:
                                all_results[detector_name] = []
                            if not events.empty:
                                all_results[detector_name].append(events)
                        
                        logger.info(f"‚úì –ü–∞–∫–µ—Ç {batch_num} –∑–∞–≤–µ—Ä—à–µ–Ω")
                        
                    except Exception as e:
                        logger.error(f"‚úó –û—à–∏–±–∫–∞ –≤ –ø–∞–∫–µ—Ç–µ {batch_num}: {e}")
        else:
            # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
            logger.info(f"–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ {len(batches)} –ø–∞–∫–µ—Ç–æ–≤...")
            
            for i, batch in enumerate(batches, 1):
                batch_results = self.process_batch(batch, detectors, output_dir, i)
                
                # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                for detector_name, events in batch_results.items():
                    if detector_name not in all_results:
                        all_results[detector_name] = []
                    if not events.empty:
                        all_results[detector_name].append(events)
        
        # –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        final_results = {}
        for detector_name, events_list in all_results.items():
            if events_list:
                combined_events = pd.concat(events_list, ignore_index=True)
                combined_events = combined_events.sort_values('ts_ns').reset_index(drop=True)
                final_results[detector_name] = combined_events
            else:
                final_results[detector_name] = pd.DataFrame()
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self._save_final_results(final_results, output_dir)
        
        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        duration = time.time() - start_time
        total_events = sum(len(df) for df in final_results.values())
        successful_detectors = sum(1 for df in final_results.values() if not df.empty)
        
        logger.info("=" * 80)
        logger.info("üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–ö–ê–ù–ò–†–û–í–ê–ù–ò–Ø:")
        logger.info(f"‚è±Ô∏è  –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {duration:.2f}—Å")
        logger.info(f"üìÅ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –ø–∞–∫–µ—Ç–æ–≤: {len(batches)}")
        logger.info(f"üîç –£—Å–ø–µ—à–Ω—ã—Ö –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤: {successful_detectors}/{len(detectors)}")
        logger.info(f"üéØ –í—Å–µ–≥–æ —Å–æ–±—ã—Ç–∏–π –Ω–∞–π–¥–µ–Ω–æ: {total_events}")
        
        for detector_name, events_df in final_results.items():
            if not events_df.empty:
                logger.info(f"  - {detector_name}: {len(events_df)} —Å–æ–±—ã—Ç–∏–π")
        
        logger.info("üéâ –ü–æ–ª–Ω–æ–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        
        return final_results
    
    def _save_final_results(self, results: Dict[str, pd.DataFrame], output_dir: Path) -> None:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        logger.info("–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
        
        summary_dir = output_dir / "summary"
        summary_dir.mkdir(parents=True, exist_ok=True)
        
        for detector_name, events_df in results.items():
            if not events_df.empty:
                summary_path = summary_dir / f"final_{detector_name.lower()}.parquet"
                events_df.to_parquet(summary_path, engine="pyarrow", index=False)
                logger.info(f"‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç {detector_name}: {summary_path}")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—â–µ–≥–æ –æ—Ç—á–µ—Ç–∞
        summary_report = {
            "scan_completed_at": datetime.now().isoformat(),
            "total_detectors": len(results),
            "detectors_with_events": sum(1 for df in results.values() if not df.empty),
            "total_events": sum(len(df) for df in results.values()),
            "detector_summary": {
                name: {"event_count": len(df), "has_events": not df.empty}
                for name, df in results.items()
            }
        }
        
        report_path = summary_dir / "scan_report.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(summary_report, f, indent=2, default=str, ensure_ascii=False)
        
        logger.info(f"‚úì –û—Ç—á–µ—Ç —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")

def run_scanner(
    config: Dict,
    detectors_config: Dict,
    data_dir: Path = Path("data/canon"),
    output_dir: Path = Path("data/events")
) -> Dict[str, pd.DataFrame]:
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–∫–∞–Ω–µ—Ä–∞"""
    logger.info("–ó–∞–ø—É—Å–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–∫–∞–Ω–µ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö...")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–∫–∞–Ω–µ—Ä–∞
    scanner = DataScanner(config)
    
    # –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
    results = scanner.run_full_scan(data_dir, output_dir, detectors_config)
    
    logger.info("–°–∫–∞–Ω–µ—Ä –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω")
    return results


if __name__ == "__main__":
    """–ó–∞–ø—É—Å–∫ –±–ª–æ–∫–∞ 13 –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    import pandas as pd
    import logging
    import sys
    import os
    import time
    
    # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ base_block
    sys.path.append(os.path.dirname(__file__))
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')
    
    print("üß™ –¢–ï–°–¢ –ë–õ–û–ö–ê 13: Scanner")
    print("=" * 50)
    
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        canon_dir = Path("../../../data/canon")
        if not canon_dir.exists():
            print("‚ùå –ü–∞–ø–∫–∞ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            print("   –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ –±–ª–æ–∫ 11 (Canonical)")
            sys.exit(1)
        
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–∞ –ø–∞–ø–∫–∞ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö: {canon_dir}")
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —Å–∫–∞–Ω–µ—Ä–∞
        config = {
            'data_types': ['quotes', 'book_top', 'features'],
            'processing': {
                'batch_size': 5,
                'max_workers': 2,
                'memory_limit_gb': 4
            },
            'filters': {
                'date_range': {},
                'exchanges': [],
                'symbols': []
            }
        }
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤
        detectors_config = {
            'D1_LiquidityVacuumBreak': {
                'threshold': 0.1,
                'empty_levels': 3
            },
            'D2_AbsorptionFlip': {
                'min_side_ratio': 0.7,
                'max_price_move_ticks': 2
            },
            'D6_BookImbalance': {
                'bid_ask_ratio': 3.0,
                'level_wall_multiplier': 5.0
            }
        }
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –±–ª–æ–∫ 13
        print("üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º –±–ª–æ–∫ 13: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Å–∫–∞–Ω–µ—Ä –¥–∞–Ω–Ω—ã—Ö...")
        start_time = time.time()
        
        results = run_scanner(config, detectors_config)
        
        execution_time = time.time() - start_time
        
        print(f"‚úÖ –ë–ª–æ–∫ 13 –≤—ã–ø–æ–ª–Ω–µ–Ω –∑–∞ {execution_time:.2f} —Å–µ–∫—É–Ω–¥!")
        print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è:")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        total_events = sum(len(df) for df in results.values())
        successful_detectors = sum(1 for df in results.values() if not df.empty)
        
        print(f"   - –£—Å–ø–µ—à–Ω—ã—Ö –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤: {successful_detectors}/{len(results)}")
        print(f"   - –í—Å–µ–≥–æ —Å–æ–±—ã—Ç–∏–π: {total_events}")
        
        for detector_name, events_df in results.items():
            if not events_df.empty:
                print(f"   - {detector_name}: {len(events_df)} —Å–æ–±—ã—Ç–∏–π")
            else:
                print(f"   - {detector_name}: —Å–æ–±—ã—Ç–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        
        print("‚úÖ –ë–ª–æ–∫ 13 —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω!")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()