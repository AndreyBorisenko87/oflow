"""
–ë–ª–æ–∫ 03: –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
–ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∫ –µ–¥–∏–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É –¥–ª—è –≤—Å–µ—Ö –±–∏—Ä–∂
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple
from pathlib import Path
import logging

logger = logging.getLogger(__name__)

def normalize_trades(
    df: pd.DataFrame,
    config: Dict = None
) -> pd.DataFrame:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è trades –¥–∞–Ω–Ω—ã—Ö –∫ –µ–¥–∏–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É
    
    Args:
        df: –°—ã—Ä—ã–µ trades –¥–∞–Ω–Ω—ã–µ
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        
    Returns:
        –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π DataFrame
    """
    if config is None:
        config = {}
    
    log_progress = config.get('log_progress', True)
    
    if log_progress:
        logger.info("–≠—Ç–∞–ø 1/3: –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è trades –¥–∞–Ω–Ω—ã—Ö")
    else:
        logger.info("–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è trades –¥–∞–Ω–Ω—ã—Ö...")
    
    if df.empty:
        logger.warning("Trades DataFrame –ø—É—Å—Ç–æ–π")
        return df
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –¥–ª—è –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏
    df_normalized = df.copy()
    
    # 1. –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è –∫–æ–ª–æ–Ω–æ–∫
    column_mapping = {
        'timestamp': 'ts_ns',
        'time': 'ts_ns',
        'ts': 'ts_ns',
        'price': 'price',
        'amount': 'size',
        'quantity': 'size',
        'qty': 'size',
        'volume': 'size',
        'side': 'side',
        'type': 'type',
        'trade_id': 'trade_id',
        'order_id': 'order_id'
    }
    
    # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    for old_name, new_name in column_mapping.items():
        if old_name in df_normalized.columns and new_name not in df_normalized.columns:
            df_normalized = df_normalized.rename(columns={old_name: new_name})
            logger.debug(f"–ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞: {old_name} -> {new_name}")
    
    # 2. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫
    if 'ts_ns' in df_normalized.columns:
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –Ω–∞–Ω–æ—Å–µ–∫—É–Ω–¥—ã –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if df_normalized['ts_ns'].dtype == 'object':
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –≤—Ä–µ–º–µ–Ω–∏
            try:
                df_normalized['ts_ns'] = pd.to_datetime(df_normalized['ts_ns']).astype(np.int64)
            except:
                logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏")
        elif df_normalized['ts_ns'].dtype in ['int64', 'int32']:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–∞–Ω–æ—Å–µ–∫—É–Ω–¥—ã (–æ–±—ã—á–Ω–æ > 1e15)
            if df_normalized['ts_ns'].max() < 1e15:
                # –í–µ—Ä–æ—è—Ç–Ω–æ —Å–µ–∫—É–Ω–¥—ã –∏–ª–∏ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥—ã
                if df_normalized['ts_ns'].max() < 1e12:  # –°–µ–∫—É–Ω–¥—ã
                    df_normalized['ts_ns'] = df_normalized['ts_ns'] * 1_000_000_000
                else:  # –ú–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥—ã
                    df_normalized['ts_ns'] = df_normalized['ts_ns'] * 1_000_000
    
    # 3. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–æ—Ä–æ–Ω—ã —Å–¥–µ–ª–∫–∏
    if 'side' in df_normalized.columns:
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        df_normalized['side'] = df_normalized['side'].str.lower()
        
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è
        side_mapping = {
            'buy': 'buy',
            'b': 'buy',
            'long': 'buy',
            'sell': 'sell',
            's': 'sell',
            'short': 'sell'
        }
        
        df_normalized['side'] = df_normalized['side'].map(side_mapping).fillna(df_normalized['side'])
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        unique_sides = df_normalized['side'].unique()
        logger.info(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã —Å–¥–µ–ª–æ–∫: {unique_sides}")
    
    # 4. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–∏–ø–æ–≤ —Å–¥–µ–ª–æ–∫
    if 'type' in df_normalized.columns:
        df_normalized['type'] = df_normalized['type'].str.lower()
        
        type_mapping = {
            'market': 'market',
            'm': 'market',
            'limit': 'limit',
            'l': 'limit'
        }
        
        df_normalized['type'] = df_normalized['type'].map(type_mapping).fillna(df_normalized['type'])
    
    # 5. –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    if 'price' in df_normalized.columns:
        # –£–¥–∞–ª—è–µ–º –∑–∞–ø–∏—Å–∏ —Å –Ω–µ–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º–∏ —Ü–µ–Ω–∞–º–∏
        invalid_prices = (df_normalized['price'] <= 0).sum()
        if invalid_prices > 0:
            logger.warning(f"–£–¥–∞–ª–µ–Ω–æ {invalid_prices} –∑–∞–ø–∏—Å–µ–π —Å –Ω–µ–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º–∏ —Ü–µ–Ω–∞–º–∏")
            df_normalized = df_normalized[df_normalized['price'] > 0]
    
    if 'size' in df_normalized.columns:
        # –£–¥–∞–ª—è–µ–º –∑–∞–ø–∏—Å–∏ —Å –Ω–µ–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º–∏ –æ–±—ä–µ–º–∞–º–∏
        invalid_sizes = (df_normalized['size'] <= 0).sum()
        if invalid_sizes > 0:
            logger.warning(f"–£–¥–∞–ª–µ–Ω–æ {invalid_sizes} –∑–∞–ø–∏—Å–µ–π —Å –Ω–µ–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º–∏ –æ–±—ä–µ–º–∞–º–∏")
            df_normalized = df_normalized[df_normalized['size'] > 0]
    
    # 6. –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
    if 'ts_ns' in df_normalized.columns:
        df_normalized = df_normalized.sort_values('ts_ns').reset_index(drop=True)
    
    logger.info(f"–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ {len(df_normalized)} trades –∑–∞–ø–∏—Å–µ–π")
    return df_normalized

def normalize_depth(
    df: pd.DataFrame,
    config: Dict = None
) -> pd.DataFrame:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è depth –¥–∞–Ω–Ω—ã—Ö –∫ –µ–¥–∏–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É
    
    Args:
        df: –°—ã—Ä—ã–µ depth –¥–∞–Ω–Ω—ã–µ
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        
    Returns:
        –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π DataFrame
    """
    if config is None:
        config = {}
    
    log_progress = config.get('log_progress', True)
    
    if log_progress:
        logger.info("–≠—Ç–∞–ø 2/3: –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è depth –¥–∞–Ω–Ω—ã—Ö")
    else:
        logger.info("–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è depth –¥–∞–Ω–Ω—ã—Ö...")
    
    if df.empty:
        logger.warning("Depth DataFrame –ø—É—Å—Ç–æ–π")
        return df
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –¥–ª—è –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏
    df_normalized = df.copy()
    
    # 1. –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è –∫–æ–ª–æ–Ω–æ–∫
    column_mapping = {
        'timestamp': 'ts_ns',
        'time': 'ts_ns',
        'ts': 'ts_ns',
        'price': 'price',
        'amount': 'size',
        'quantity': 'size',
        'qty': 'size',
        'volume': 'size',
        'side': 'side',
        'level': 'level',
        'order_id': 'order_id',
        'update_type': 'update_type'
    }
    
    # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    for old_name, new_name in column_mapping.items():
        if old_name in df_normalized.columns and new_name not in df_normalized.columns:
            df_normalized = df_normalized.rename(columns={old_name: new_name})
            logger.debug(f"–ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞: {old_name} -> {new_name}")
    
    # 2. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫
    if 'ts_ns' in df_normalized.columns:
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –Ω–∞–Ω–æ—Å–µ–∫—É–Ω–¥—ã –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if df_normalized['ts_ns'].dtype == 'object':
            try:
                df_normalized['ts_ns'] = pd.to_datetime(df_normalized['ts_ns']).astype(np.int64)
            except:
                logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏")
        elif df_normalized['ts_ns'].dtype in ['int64', 'int32']:
            if df_normalized['ts_ns'].max() < 1e15:
                if df_normalized['ts_ns'].max() < 1e12:  # –°–µ–∫—É–Ω–¥—ã
                    df_normalized['ts_ns'] = df_normalized['ts_ns'] * 1_000_000_000
                else:  # –ú–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥—ã
                    df_normalized['ts_ns'] = df_normalized['ts_ns'] * 1_000_000
    
    # 3. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–æ—Ä–æ–Ω—ã
    if 'side' in df_normalized.columns:
        df_normalized['side'] = df_normalized['side'].str.lower()
        
        side_mapping = {
            'bid': 'bid',
            'b': 'bid',
            'buy': 'bid',
            'ask': 'ask',
            'a': 'ask',
            'sell': 'ask'
        }
        
        df_normalized['side'] = df_normalized['side'].map(side_mapping).fillna(df_normalized['side'])
    
    # 4. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–∏–ø–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
    if 'update_type' in df_normalized.columns:
        df_normalized['update_type'] = df_normalized['update_type'].str.lower()
        
        type_mapping = {
            'new': 'new',
            'update': 'update',
            'delete': 'delete',
            'remove': 'delete'
        }
        
        df_normalized['update_type'] = df_normalized['update_type'].map(type_mapping).fillna(df_normalized['update_type'])
    
    # 5. –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    if 'price' in df_normalized.columns:
        invalid_prices = (df_normalized['price'] <= 0).sum()
        if invalid_prices > 0:
            logger.warning(f"–£–¥–∞–ª–µ–Ω–æ {invalid_prices} –∑–∞–ø–∏—Å–µ–π —Å –Ω–µ–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º–∏ —Ü–µ–Ω–∞–º–∏")
            df_normalized = df_normalized[df_normalized['price'] > 0]
    
    if 'size' in df_normalized.columns:
        # –î–ª—è depth —Ä–∞–∑–º–µ—Ä –º–æ–∂–µ—Ç –±—ã—Ç—å 0 (—É–¥–∞–ª–µ–Ω–∏–µ —É—Ä–æ–≤–Ω—è)
        negative_sizes = (df_normalized['size'] < 0).sum()
        if negative_sizes > 0:
            logger.warning(f"–£–¥–∞–ª–µ–Ω–æ {negative_sizes} –∑–∞–ø–∏—Å–µ–π —Å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º–∏ –æ–±—ä–µ–º–∞–º–∏")
            df_normalized = df_normalized[df_normalized['size'] >= 0]
    
    # 6. –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
    if 'ts_ns' in df_normalized.columns:
        df_normalized = df_normalized.sort_values('ts_ns').reset_index(drop=True)
    
    logger.info(f"–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ {len(df_normalized)} depth –∑–∞–ø–∏—Å–µ–π")
    return df_normalized

def validate_normalized_data(
    trades_df: pd.DataFrame,
    depth_df: pd.DataFrame,
    config: Dict = None
) -> Dict[str, float]:
    """
    –í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    
    Args:
        trades_df: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ trades
        depth_df: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ depth
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∫–∞—á–µ—Å—Ç–≤–∞
    """
    if config is None:
        config = {}
    
    log_progress = config.get('log_progress', True)
    
    if log_progress:
        logger.info("–≠—Ç–∞–ø 3/3: –í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
    else:
        logger.info("–í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    
    quality_metrics = {}
    
    # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ trades
    if not trades_df.empty:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        required_trades_columns = ['ts_ns', 'price', 'size', 'side']
        missing_trades_columns = [col for col in required_trades_columns if col not in trades_df.columns]
        
        if missing_trades_columns:
            logger.warning(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–ª–æ–Ω–∫–∏ –≤ trades: {missing_trades_columns}")
            trades_completeness = (len(required_trades_columns) - len(missing_trades_columns)) / len(required_trades_columns)
        else:
            trades_completeness = 1.0
        
        quality_metrics["trades_completeness"] = trades_completeness
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö
        if 'ts_ns' in trades_df.columns:
            is_timestamp_numeric = pd.api.types.is_numeric_dtype(trades_df['ts_ns'])
            quality_metrics["trades_timestamp_valid"] = 1.0 if is_timestamp_numeric else 0.0
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–æ–≥–∏—á–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
        if 'price' in trades_df.columns and 'size' in trades_df.columns:
            valid_prices = (trades_df['price'] > 0).sum()
            valid_sizes = (trades_df['size'] > 0).sum()
            total_trades = len(trades_df)
            
            quality_metrics["trades_price_valid"] = valid_prices / total_trades if total_trades > 0 else 0.0
            quality_metrics["trades_size_valid"] = valid_sizes / total_trades if total_trades > 0 else 0.0
    else:
        quality_metrics["trades_completeness"] = 0.0
        quality_metrics["trades_timestamp_valid"] = 0.0
        quality_metrics["trades_price_valid"] = 0.0
        quality_metrics["trades_size_valid"] = 0.0
    
    # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ depth
    if not depth_df.empty:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        required_depth_columns = ['ts_ns', 'price', 'size', 'side']
        missing_depth_columns = [col for col in required_depth_columns if col not in depth_df.columns]
        
        if missing_depth_columns:
            logger.warning(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–ª–æ–Ω–∫–∏ –≤ depth: {missing_depth_columns}")
            depth_completeness = (len(required_depth_columns) - len(missing_depth_columns)) / len(required_depth_columns)
        else:
            depth_completeness = 1.0
        
        quality_metrics["depth_completeness"] = depth_completeness
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö
        if 'ts_ns' in depth_df.columns:
            is_timestamp_numeric = pd.api.types.is_numeric_dtype(depth_df['ts_ns'])
            quality_metrics["depth_timestamp_valid"] = 1.0 if is_timestamp_numeric else 0.0
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–æ–≥–∏—á–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
        if 'price' in depth_df.columns and 'size' in depth_df.columns:
            valid_prices = (depth_df['price'] > 0).sum()
            valid_sizes = (depth_df['size'] >= 0).sum()  # –†–∞–∑–º–µ—Ä –º–æ–∂–µ—Ç –±—ã—Ç—å 0
            total_depth = len(depth_df)
            
            quality_metrics["depth_price_valid"] = valid_prices / total_depth if total_depth > 0 else 0.0
            quality_metrics["depth_size_valid"] = valid_sizes / total_depth if total_depth > 0 else 0.0
    else:
        quality_metrics["depth_completeness"] = 0.0
        quality_metrics["depth_timestamp_valid"] = 0.0
        quality_metrics["depth_price_valid"] = 0.0
        quality_metrics["depth_size_valid"] = 0.0
    
    # 3. –û–±—â–∏–π —Å–∫–æ—Ä –∫–∞—á–µ—Å—Ç–≤–∞
    overall_quality = np.mean(list(quality_metrics.values()))
    quality_metrics["overall_quality"] = overall_quality
    
    # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    logger.info("=== –ú–ï–¢–†–ò–ö–ò –ö–ê–ß–ï–°–¢–í–ê –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–ò ===")
    for metric, score in quality_metrics.items():
        logger.info(f"{metric}: {score:.3f}")
    
    # –¶–≤–µ—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
    if overall_quality >= 0.9:
        logger.info("üü¢ –ö–∞—á–µ—Å—Ç–≤–æ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: –û–¢–õ–ò–ß–ù–û–ï")
    elif overall_quality >= 0.7:
        logger.info("üü° –ö–∞—á–µ—Å—Ç–≤–æ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: –•–û–†–û–®–ï–ï")
    elif overall_quality >= 0.5:
        logger.info("üü† –ö–∞—á–µ—Å—Ç–≤–æ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: –£–î–û–í–õ–ï–¢–í–û–†–ò–¢–ï–õ–¨–ù–û–ï")
    else:
        logger.warning("üî¥ –ö–∞—á–µ—Å—Ç–≤–æ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: –ü–õ–û–•–û–ï")
    
    return quality_metrics

def run_normalization(
    trades_df: pd.DataFrame,
    depth_df: pd.DataFrame,
    config: Dict
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö
    
    Args:
        trades_df: –°—ã—Ä—ã–µ trades –¥–∞–Ω–Ω—ã–µ
        depth_df: –°—ã—Ä—ã–µ depth –¥–∞–Ω–Ω—ã–µ
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        
    Returns:
        –ö–æ—Ä—Ç–µ–∂ (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ trades, –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ depth)
    """
    logger.info("–ó–∞–ø—É—Å–∫ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö...")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    norm_config = config.get('normalization', {})
    log_progress = norm_config.get('log_progress', True)
    
    try:
        # 1. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è trades
        normalized_trades = normalize_trades(trades_df, norm_config)
        
        # 2. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è depth
        normalized_depth = normalize_depth(depth_df, norm_config)
        
        # 3. –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
        quality = validate_normalized_data(normalized_trades, normalized_depth, norm_config)
        
        # 4. –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        logger.info("=== –ò–¢–û–ì–ò –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–ò ===")
        logger.info(f"–ò—Å—Ö–æ–¥–Ω—ã–µ trades: {len(trades_df):,}")
        logger.info(f"–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ trades: {len(normalized_trades):,}")
        logger.info(f"–ò—Å—Ö–æ–¥–Ω—ã–µ depth: {len(depth_df):,}")
        logger.info(f"–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ depth: {len(normalized_depth):,}")
        logger.info(f"–û–±—â–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ: {quality.get('overall_quality', 0):.3f}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –±–∏—Ä–∂–∞–º
        if 'exchange' in normalized_trades.columns:
            for exchange in normalized_trades['exchange'].unique():
                ex_count = len(normalized_trades[normalized_trades['exchange'] == exchange])
                logger.info(f"  {exchange} trades: {ex_count:,}")
        
        if 'exchange' in normalized_depth.columns:
            for exchange in normalized_depth['exchange'].unique():
                ex_count = len(normalized_depth[normalized_depth['exchange'] == exchange])
                logger.info(f"  {exchange} depth: {ex_count:,}")
        
        logger.info("‚úÖ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
        raise
    
    return normalized_trades, normalized_depth
