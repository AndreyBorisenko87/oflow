"""
–ë–ª–æ–∫ 05: –õ—É—á—à–∏–µ —Ü–µ–Ω—ã
–í–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å best bid/ask/mid/—Å–ø—Ä–µ–¥ —Å —à–∞–≥–æ–º 10 –º—Å
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple
from pathlib import Path
import logging

logger = logging.getLogger(__name__)

def restore_quotes(
    depth_df: pd.DataFrame,
    tick_size_ms: int = 10,
    config: Dict = None
) -> pd.DataFrame:
    """
    –í–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å best bid/ask/mid —Ü–µ–Ω—ã –∏–∑ depth –¥–∞–Ω–Ω—ã—Ö
    
    Args:
        depth_df: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ depth –¥–∞–Ω–Ω—ã–µ
        tick_size_ms: –®–∞–≥ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Ü–µ–Ω
        
    Returns:
        DataFrame —Å quotes (ts_ns, exchange, best_bid, best_ask, mid, spread)
    """
    if config is None:
        config = {}
    
    log_progress = config.get('log_progress', True)
    
    if log_progress:
        logger.info("–≠—Ç–∞–ø 1/4: –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ best bid/ask/mid —Ü–µ–Ω")
    else:
        logger.info("–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ best bid/ask/mid —Ü–µ–Ω...")
    
    if depth_df.empty:
        logger.warning("Depth DataFrame –ø—É—Å—Ç–æ–π, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
        return pd.DataFrame(columns=[
            'ts_ns', 'exchange', 'best_bid', 'best_ask', 'mid', 'spread'
        ])
    
    # 1. –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫–Ω–∞ –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
    tick_size_ns = tick_size_ms * 1_000_000  # –ø–µ—Ä–µ–≤–æ–¥–∏–º –≤ –Ω–∞–Ω–æ—Å–µ–∫—É–Ω–¥—ã
    
    # –û–∫—Ä—É–≥–ª—è–µ–º timestamps –¥–æ –≥—Ä–∞–Ω–∏—Ü —Ç–∏–∫–æ–≤
    depth_df = depth_df.copy()
    depth_df['time_bucket'] = (depth_df['ts_ns'] // tick_size_ns) * tick_size_ns
    
    logger.info(f"–ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ —Ç–∏–∫–∞–º {tick_size_ms}ms ({tick_size_ns}–Ω—Å)")
    
    # 2. –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–º –æ–∫–Ω–∞–º –∏ –±–∏—Ä–∂–∞–º
    quotes_list = []
    
    for exchange in depth_df['exchange'].unique():
        ex_data = depth_df[depth_df['exchange'] == exchange]
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–º –æ–∫–Ω–∞–º
        for bucket_time in ex_data['time_bucket'].unique():
            bucket_data = ex_data[ex_data['time_bucket'] == bucket_time]
            
            # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ bid –∏ ask
            bids = bucket_data[bucket_data['side'] == 'bid']
            asks = bucket_data[bucket_data['side'] == 'ask']
            
            best_bid = None
            best_ask = None
            
            # 3. –ù–∞—Ö–æ–¥–∏–º best bid (–º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞)
            if not bids.empty:
                # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∞–∫—Ç–∏–≤–Ω—ã–µ —É—Ä–æ–≤–Ω–∏ (size > 0)
                active_bids = bids[bids['size'] > 0]
                if not active_bids.empty:
                    best_bid = active_bids['price'].max()
            
            # 4. –ù–∞—Ö–æ–¥–∏–º best ask (–º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞)
            if not asks.empty:
                # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∞–∫—Ç–∏–≤–Ω—ã–µ —É—Ä–æ–≤–Ω–∏ (size > 0)
                active_asks = asks[asks['size'] > 0]
                if not active_asks.empty:
                    best_ask = active_asks['price'].min()
            
            # 5. –°–æ–∑–¥–∞–µ–º quote —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –∏ bid –∏ ask
            if best_bid is not None and best_ask is not None:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–æ–≥–∏—á–Ω–æ—Å—Ç—å (bid < ask)
                if best_bid < best_ask:
                    mid = (best_bid + best_ask) / 2
                    spread = best_ask - best_bid
                    
                    quotes_list.append({
                        'ts_ns': bucket_time,
                        'exchange': exchange,
                        'best_bid': best_bid,
                        'best_ask': best_ask,
                        'mid': mid,
                        'spread': spread
                    })
                else:
                    logger.debug(f"–ü—Ä–æ–ø—É—â–µ–Ω –Ω–µ–ª–æ–≥–∏—á–Ω—ã–π quote: bid={best_bid}, ask={best_ask}")
            else:
                logger.debug(f"–ü—Ä–æ–ø—É—â–µ–Ω –Ω–µ–ø–æ–ª–Ω—ã–π quote: bid={best_bid}, ask={best_ask}")
    
    # 6. –°–æ–∑–¥–∞–µ–º DataFrame –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏
    if quotes_list:
        quotes_df = pd.DataFrame(quotes_list)
        quotes_df = quotes_df.sort_values('ts_ns').reset_index(drop=True)
        
        exchanges_count = len(quotes_df['exchange'].unique())
        logger.info(f"–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ {len(quotes_df)} quotes –¥–ª—è {exchanges_count} –±–∏—Ä–∂")
        logger.info(f"–í—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω: {quotes_df['ts_ns'].min()} - {quotes_df['ts_ns'].max()}")
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –±–∏—Ä–∂–∞–º
        for exchange in quotes_df['exchange'].unique():
            ex_quotes = quotes_df[quotes_df['exchange'] == exchange]
            logger.info(f"  {exchange}: {len(ex_quotes)} quotes")
    else:
        quotes_df = pd.DataFrame(columns=[
            'ts_ns', 'exchange', 'best_bid', 'best_ask', 'mid', 'spread'
        ])
        logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ quote")
    
    return quotes_df

def calculate_spreads(quotes_df: pd.DataFrame) -> pd.DataFrame:
    """
    –í—ã—á–∏—Å–ª–∏—Ç—å —Å–ø—Ä–µ–¥—ã –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    
    Args:
        quotes_df: DataFrame —Å quotes
        
    Returns:
        DataFrame —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏ —Å–ø—Ä–µ–¥–æ–≤
    """
    logger.info("–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–ø—Ä–µ–¥–æ–≤...")
    
    if quotes_df.empty:
        return quotes_df
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –¥–ª—è –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏
    quotes_df = quotes_df.copy()
    
    # 1. –ê–±—Å–æ–ª—é—Ç–Ω—ã–π —Å–ø—Ä–µ–¥ (ask - bid) - —É–∂–µ –µ—Å—Ç—å –≤ –∫–æ–ª–æ–Ω–∫–µ 'spread'
    quotes_df['spread_abs'] = quotes_df['spread']
    
    # 2. –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π —Å–ø—Ä–µ–¥ (%)
    quotes_df['spread_rel'] = quotes_df['spread'] / quotes_df['mid'] * 100
    
    # 3. –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ —Å–ø—Ä–µ–¥–æ–≤ (–ø–æ –±–∏—Ä–∂–∞–º)
    for exchange in quotes_df['exchange'].unique():
        ex_mask = quotes_df['exchange'] == exchange
        ex_data = quotes_df[ex_mask].copy()
        
        if len(ex_data) > 0:
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏
            ex_data = ex_data.sort_values('ts_ns')
            
            # –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ –¥–ª—è –∞–±—Å–æ–ª—é—Ç–Ω–æ–≥–æ —Å–ø—Ä–µ–¥–∞
            quotes_df.loc[ex_mask, 'spread_ma_5'] = ex_data['spread'].rolling(window=5, min_periods=1).mean()
            quotes_df.loc[ex_mask, 'spread_ma_20'] = ex_data['spread'].rolling(window=20, min_periods=1).mean()
            
            # –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ –¥–ª—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Å–ø—Ä–µ–¥–∞
            quotes_df.loc[ex_mask, 'spread_rel_ma_5'] = ex_data['spread_rel'].rolling(window=5, min_periods=1).mean()
            quotes_df.loc[ex_mask, 'spread_rel_ma_20'] = ex_data['spread_rel'].rolling(window=20, min_periods=1).mean()
    
    # 4. –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å —Å–ø—Ä–µ–¥–æ–≤ (—Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ)
    for exchange in quotes_df['exchange'].unique():
        ex_mask = quotes_df['exchange'] == exchange
        ex_data = quotes_df[ex_mask].copy()
        
        if len(ex_data) > 0:
            ex_data = ex_data.sort_values('ts_ns')
            
            # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –∞–±—Å–æ–ª—é—Ç–Ω–æ–≥–æ —Å–ø—Ä–µ–¥–∞
            quotes_df.loc[ex_mask, 'spread_volatility'] = ex_data['spread'].rolling(window=20, min_periods=1).std()
            
            # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Å–ø—Ä–µ–¥–∞
            quotes_df.loc[ex_mask, 'spread_rel_volatility'] = ex_data['spread_rel'].rolling(window=20, min_periods=1).std()
    
    # 5. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    quotes_df['bid_ask_ratio'] = quotes_df['best_bid'] / quotes_df['best_ask']
    quotes_df['mid_change'] = quotes_df['mid'].diff()
    quotes_df['mid_change_pct'] = quotes_df['mid_change'] / quotes_df['mid'].shift(1) * 100
    
    # 6. –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    logger.info("–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–ø—Ä–µ–¥–æ–≤:")
    logger.info(f"  –ê–±—Å–æ–ª—é—Ç–Ω—ã–π —Å–ø—Ä–µ–¥: {quotes_df['spread'].min():.4f} - {quotes_df['spread'].max():.4f}")
    logger.info(f"  –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π —Å–ø—Ä–µ–¥: {quotes_df['spread_rel'].min():.4f}% - {quotes_df['spread_rel'].max():.4f}%")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –±–∏—Ä–∂–∞–º
    for exchange in quotes_df['exchange'].unique():
        ex_data = quotes_df[quotes_df['exchange'] == exchange]
        logger.info(f"  {exchange}: —Å—Ä–µ–¥–Ω–∏–π —Å–ø—Ä–µ–¥ {ex_data['spread'].mean():.4f} ({ex_data['spread_rel'].mean():.4f}%)")
    
    return quotes_df

def aggregate_by_tick(
    quotes_df: pd.DataFrame,
    tick_size_ms: int = 10
) -> pd.DataFrame:
    """
    –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞—Ç—å quotes –ø–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–º —Ç–∏–∫–∞–º
    
    Args:
        quotes_df: DataFrame —Å quotes
        tick_size_ms: –†–∞–∑–º–µ—Ä —Ç–∏–∫–∞ –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö
        
    Returns:
        –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π DataFrame
    """
    logger.info(f"–ê–≥—Ä–µ–≥–∞—Ü–∏—è quotes –ø–æ —Ç–∏–∫–∞–º {tick_size_ms}ms...")
    
    if quotes_df.empty:
        return quotes_df
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –¥–ª—è –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏
    quotes_df = quotes_df.copy()
    
    # 1. –û–∫—Ä—É–≥–ª–∏—Ç—å timestamps –¥–æ –≥—Ä–∞–Ω–∏—Ü —Ç–∏–∫–æ–≤
    tick_size_ns = tick_size_ms * 1_000_000
    quotes_df['tick_bucket'] = (quotes_df['ts_ns'] // tick_size_ns) * tick_size_ns
    
    logger.info(f"–°–æ–∑–¥–∞–Ω–æ {quotes_df['tick_bucket'].nunique()} —Ç–∏–∫–æ–≤—ã—Ö –æ–∫–æ–Ω")
    
    # 2. –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–≥–æ —Ç–∏–∫–∞
    aggregated_quotes = []
    
    for exchange in quotes_df['exchange'].unique():
        ex_data = quotes_df[quotes_df['exchange'] == exchange]
        
        for tick_time in ex_data['tick_bucket'].unique():
            tick_data = ex_data[ex_data['tick_bucket'] == tick_time]
            
            if len(tick_data) > 0:
                # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –≤–Ω—É—Ç—Ä–∏ —Ç–∏–∫–∞
                aggregated_quote = {
                    'ts_ns': tick_time,
                    'exchange': exchange,
                    'best_bid': tick_data['best_bid'].mean(),  # –°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                    'best_ask': tick_data['best_ask'].mean(),
                    'mid': tick_data['mid'].mean(),
                    'spread': tick_data['spread'].mean(),
                    'spread_abs': tick_data['spread_abs'].mean(),
                    'spread_rel': tick_data['spread_rel'].mean(),
                    'quotes_count': len(tick_data)  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ quotes –≤ —Ç–∏–∫–µ
                }
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Å–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ –µ—Å–ª–∏ –µ—Å—Ç—å
                if 'spread_ma_5' in tick_data.columns:
                    aggregated_quote['spread_ma_5'] = tick_data['spread_ma_5'].mean()
                    aggregated_quote['spread_ma_20'] = tick_data['spread_ma_5'].mean()
                    aggregated_quote['spread_rel_ma_5'] = tick_data['spread_rel_ma_5'].mean()
                    aggregated_quote['spread_rel_ma_20'] = tick_data['spread_rel_ma_20'].mean()
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –µ—Å–ª–∏ –µ—Å—Ç—å
                if 'spread_volatility' in tick_data.columns:
                    aggregated_quote['spread_volatility'] = tick_data['spread_volatility'].mean()
                    aggregated_quote['spread_rel_volatility'] = tick_data['spread_rel_volatility'].mean()
                
                # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –µ—Å–ª–∏ –µ—Å—Ç—å
                if 'bid_ask_ratio' in tick_data.columns:
                    aggregated_quote['bid_ask_ratio'] = tick_data['bid_ask_ratio'].mean()
                    aggregated_quote['mid_change'] = tick_data['mid_change'].sum()  # –°—É–º–º–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π
                    aggregated_quote['mid_change_pct'] = tick_data['mid_change_pct'].mean()
                
                aggregated_quotes.append(aggregated_quote)
    
    # 3. –°–æ–∑–¥–∞–µ–º –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π DataFrame
    if aggregated_quotes:
        aggregated_df = pd.DataFrame(aggregated_quotes)
        aggregated_df = aggregated_df.sort_values('ts_ns').reset_index(drop=True)
        
        logger.info(f"–ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–æ {len(aggregated_df)} —Ç–∏–∫–æ–≤—ã—Ö quotes")
        logger.info(f"–°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ quotes –Ω–∞ —Ç–∏–∫: {aggregated_df['quotes_count'].mean():.1f}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –±–∏—Ä–∂–∞–º
        for exchange in aggregated_df['exchange'].unique():
            ex_data = aggregated_df[aggregated_df['exchange'] == exchange]
            logger.info(f"  {exchange}: {len(ex_data)} —Ç–∏–∫–æ–≤—ã—Ö quotes")
    else:
        aggregated_df = pd.DataFrame()
        logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞—Ç—å quotes")
    
    return aggregated_df

def validate_quotes(quotes_df: pd.DataFrame) -> Dict[str, float]:
    """
    –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö quotes
    
    Args:
        quotes_df: DataFrame —Å quotes
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∫–∞—á–µ—Å—Ç–≤–∞
    """
    logger.info("–í–∞–ª–∏–¥–∞—Ü–∏—è quotes...")
    
    if quotes_df.empty:
        return {"quality": 0.0, "completeness": 0.0}
    
    quality_metrics = {}
    
    # 1. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ª–æ–≥–∏—á–Ω–æ—Å—Ç—å —Ü–µ–Ω (bid < ask)
    valid_bid_ask = (quotes_df['best_bid'] < quotes_df['best_ask']).sum()
    total_quotes = len(quotes_df)
    bid_ask_validity = valid_bid_ask / total_quotes if total_quotes > 0 else 0
    
    quality_metrics["bid_ask_valid"] = bid_ask_validity
    
    if bid_ask_validity < 1.0:
        invalid_count = total_quotes - valid_bid_ask
        logger.warning(f"–ù–∞–π–¥–µ–Ω–æ {invalid_count} –Ω–µ–ª–æ–≥–∏—á–Ω—ã—Ö quotes (bid >= ask)")
    
    # 2. –û—Ü–µ–Ω–∏—Ç—å –ø–æ–ª–Ω–æ—Ç—É –¥–∞–Ω–Ω—ã—Ö
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
    required_columns = ['ts_ns', 'exchange', 'best_bid', 'best_ask', 'mid', 'spread']
    missing_columns = [col for col in required_columns if col not in quotes_df.columns]
    
    if missing_columns:
        logger.warning(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–ª–æ–Ω–∫–∏: {missing_columns}")
        completeness = (len(required_columns) - len(missing_columns)) / len(required_columns)
    else:
        completeness = 1.0
    
    quality_metrics["completeness"] = completeness
    
    # 3. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä–∞–∑—Ä—ã–≤—ã
    if len(quotes_df) > 1:
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        sorted_quotes = quotes_df.sort_values('ts_ns')
        
        # –í—ã—á–∏—Å–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã –º–µ–∂–¥—É quotes
        time_diffs = sorted_quotes['ts_ns'].diff().dropna()
        
        if len(time_diffs) > 0:
            # –ù–∞—Ö–æ–¥–∏–º –±–æ–ª—å—à–∏–µ —Ä–∞–∑—Ä—ã–≤—ã (> 1 —Å–µ–∫—É–Ω–¥—ã)
            large_gaps = (time_diffs > 1_000_000_000).sum()
            gap_ratio = large_gaps / len(time_diffs)
            
            quality_metrics["time_coverage"] = 1.0 - gap_ratio
            
            if large_gaps > 0:
                logger.warning(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {large_gaps} –±–æ–ª—å—à–∏—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä–∞–∑—Ä—ã–≤–æ–≤ (>1—Å)")
        else:
            quality_metrics["time_coverage"] = 1.0
    else:
        quality_metrics["time_coverage"] = 1.0
    
    # 4. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ —Å–ø—Ä–µ–¥–æ–≤
    if 'spread' in quotes_df.columns:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ —Å–ø—Ä–µ–¥—ã
        negative_spreads = (quotes_df['spread'] < 0).sum()
        if negative_spreads > 0:
            logger.warning(f"–ù–∞–π–¥–µ–Ω–æ {negative_spreads} –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–ø—Ä–µ–¥–æ–≤")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª—å—à–∏–µ —Å–ø—Ä–µ–¥—ã (>10% –æ—Ç —Ü–µ–Ω—ã)
        if 'spread_rel' in quotes_df.columns:
            large_spreads = (quotes_df['spread_rel'] > 10.0).sum()
            if large_spreads > 0:
                logger.warning(f"–ù–∞–π–¥–µ–Ω–æ {large_spreads} –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª—å—à–∏—Ö —Å–ø—Ä–µ–¥–æ–≤ (>10%)")
    
    # 5. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –±–∏—Ä–∂–∞–º
    exchange_counts = quotes_df['exchange'].value_counts()
    if len(exchange_counts) > 1:
        # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        total = exchange_counts.sum()
        expected_per_exchange = total / len(exchange_counts)
        variance = ((exchange_counts - expected_per_exchange) ** 2).sum() / len(exchange_counts)
        distribution_score = max(0, 1.0 - variance / (expected_per_exchange ** 2))
        
        quality_metrics["distribution_balance"] = distribution_score
        
        if distribution_score < 0.8:
            logger.warning(f"–ù–µ—Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –±–∏—Ä–∂–∞–º: {exchange_counts.to_dict()}")
    else:
        quality_metrics["distribution_balance"] = 1.0
    
    # 6. –û–±—â–∏–π —Å–∫–æ—Ä –∫–∞—á–µ—Å—Ç–≤–∞
    overall_quality = np.mean(list(quality_metrics.values()))
    quality_metrics["overall_quality"] = overall_quality
    
    # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    logger.info("=== –ú–ï–¢–†–ò–ö–ò –ö–ê–ß–ï–°–¢–í–ê QUOTES ===")
    for metric, score in quality_metrics.items():
        logger.info(f"{metric}: {score:.3f}")
    
    # –¶–≤–µ—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
    if overall_quality >= 0.9:
        logger.info("üü¢ –ö–∞—á–µ—Å—Ç–≤–æ quotes: –û–¢–õ–ò–ß–ù–û–ï")
    elif overall_quality >= 0.7:
        logger.info("üü° –ö–∞—á–µ—Å—Ç–≤–æ quotes: –•–û–†–û–®–ï–ï")
    elif overall_quality >= 0.5:
        logger.info("üü† –ö–∞—á–µ—Å—Ç–≤–æ quotes: –£–î–û–í–õ–ï–¢–í–û–†–ò–¢–ï–õ–¨–ù–û–ï")
    else:
        logger.warning("üî¥ –ö–∞—á–µ—Å—Ç–≤–æ quotes: –ü–õ–û–•–û–ï")
    
    return quality_metrics

def save_quotes(
    quotes_df: pd.DataFrame,
    output_path: Path,
    config: Dict
) -> None:
    """
    –°–æ—Ö—Ä–∞–Ω–∏—Ç—å quotes –≤ parquet —Ñ–∞–π–ª
    
    Args:
        quotes_df: DataFrame —Å quotes
        output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    """
    logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ quotes –≤ {output_path}...")
    
    if quotes_df.empty:
        logger.warning("Quotes DataFrame –ø—É—Å—Ç–æ–π, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–æ")
        return
    
    try:
        # 1. –°–æ–∑–¥–∞—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # 2. –ü–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–∂–∞—Ç–∏—è –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        compression = config.get("export", {}).get("compression", "snappy")
        
        # 3. –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ parquet —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
        quotes_df.to_parquet(
            output_path, 
            engine="pyarrow", 
            compression=compression,
            index=False
        )
        
        # 4. –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏
        file_size_mb = output_path.stat().st_size / (1024 * 1024)
        logger.info(f"Quotes —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {len(quotes_df)} —Å—Ç—Ä–æ–∫")
        logger.info(f"–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_size_mb:.1f} MB")
        logger.info(f"–ü—É—Ç—å: {output_path}")
        
        # 5. –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –±–∏—Ä–∂–∞–º
        if 'exchange' in quotes_df.columns:
            exchange_counts = quotes_df['exchange'].value_counts()
            logger.info("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –±–∏—Ä–∂–∞–º:")
            for exchange, count in exchange_counts.items():
                logger.info(f"  {exchange}: {count:,} quotes")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ quotes: {e}")
        raise

def run_best_prices(
    depth_df: pd.DataFrame,
    config: Dict,
    output_dir: Path = Path("data/quotes")
) -> pd.DataFrame:
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –ª—É—á—à–∏—Ö —Ü–µ–Ω
    
    Args:
        depth_df: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ depth –¥–∞–Ω–Ω—ã–µ
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        
    Returns:
        DataFrame —Å quotes
    """
    logger.info("–ó–∞–ø—É—Å–∫ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –ª—É—á—à–∏—Ö —Ü–µ–Ω...")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    best_prices_config = config.get('best_prices', {})
    log_progress = best_prices_config.get('log_progress', True)
    
    try:
        if log_progress:
            logger.info("–≠—Ç–∞–ø 1/4: –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ quotes –∏–∑ depth –¥–∞–Ω–Ω—ã—Ö")
        
        # 1. –í–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å quotes
        tick_size_ms = best_prices_config.get("tick_size_ms", 10)
        logger.info(f"–®–∞–≥ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏: {tick_size_ms}ms")
        
        quotes_df = restore_quotes(depth_df, tick_size_ms, best_prices_config)
        
        if quotes_df.empty:
            logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å quotes, –∑–∞–≤–µ—Ä—à–∞–µ–º —Ä–∞–±–æ—Ç—É")
            return quotes_df
        
        if log_progress:
            logger.info("–≠—Ç–∞–ø 2/4: –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–ø—Ä–µ–¥–æ–≤ –∏ –º–µ—Ç—Ä–∏–∫")
        
        # 2. –í—ã—á–∏—Å–ª–∏—Ç—å —Å–ø—Ä–µ–¥—ã –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        quotes_df = calculate_spreads(quotes_df, best_prices_config)
        
        if log_progress:
            logger.info("–≠—Ç–∞–ø 3/4: –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–º —Ç–∏–∫–∞–º")
        
        # 3. –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞—Ç—å –ø–æ —Ç–∏–∫–∞–º
        quotes_df = aggregate_by_tick(quotes_df, tick_size_ms)
        
        if quotes_df.empty:
            logger.warning("–ê–≥—Ä–µ–≥–∞—Ü–∏—è –Ω–µ –¥–∞–ª–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            return quotes_df
        
        if log_progress:
            logger.info("–≠—Ç–∞–ø 4/4: –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ")
        
        # 4. –í–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ
        quality = validate_quotes(quotes_df)
        
        # 5. –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        output_path = output_dir / "quotes.parquet"
        save_quotes(quotes_df, output_path, config)
        
        # 6. –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        logger.info("=== –ò–¢–û–ì–ò –í–û–°–°–¢–ê–ù–û–í–õ–ï–ù–ò–Ø QUOTES ===")
        logger.info(f"–ò—Å—Ö–æ–¥–Ω—ã–µ depth –∑–∞–ø–∏—Å–∏: {len(depth_df):,}")
        logger.info(f"–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ quotes: {len(quotes_df):,}")
        logger.info(f"–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è: {len(quotes_df)/len(depth_df)*100:.2f}%")
        logger.info(f"–û–±—â–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ: {quality.get('overall_quality', 0):.3f}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –±–∏—Ä–∂–∞–º
        if 'exchange' in quotes_df.columns:
            for exchange in quotes_df['exchange'].unique():
                ex_count = len(quotes_df[quotes_df['exchange'] == exchange])
                logger.info(f"  {exchange}: {ex_count:,} quotes")
        
        logger.info("‚úÖ –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ª—É—á—à–∏—Ö —Ü–µ–Ω –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–∏ quotes: {e}")
        raise
    
    return quotes_df
